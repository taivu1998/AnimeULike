# -*- coding: utf-8 -*-
"""DEEPANIGNET WITH EVALUATION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MdKicPe4R2HNnKn6JlOeQDeol4vq55EW
"""

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
os.environ['HOME']="dfs/user/msun415"


DEVICE_IDS=[0,1,2]
os.environ['CUDA_VISIBLE_DEVICES']=",".join(list(map(str,DEVICE_IDS)))
device = "cuda:0"
BATCH_SIZE,NUM_WORKERS=60,30

PATH="models/citeulike/bert/bert_finetune/activation=tanh__anime_dim=768__hidden_dims=[800, 400]__item_drop_p=0.5__latent_dim=200__user_drop_p=0.5.pt_0.pt"

import pandas as pd
from collections import OrderedDict
from torch.utils.data import Dataset

import numpy as np
from abc import abstractmethod
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from typing import List
import torch
from torch.nn import functional as F

import transformers
import torch.nn as nn
import itertools
from torch.utils.data import DataLoader


# Helper function for visualization.
# %matplotlib inline
import torch
import matplotlib.pyplot as plt




TRANSFORMER_PRETRAINED_MODELS = {
    'bert-base-uncased': {'tokenizer': transformers.BertTokenizerFast, 'model': transformers.BertModel},
    'distilbert-base-uncased': {'tokenizer': transformers.DistilBertTokenizerFast, 'model': transformers.DistilBertModel},
    'roberta-base': {'tokenizer': transformers.RobertaTokenizer, 'model': transformers.RobertaModel},
    'google/electra-small-discriminator': {'tokenizer': transformers.ElectraTokenizer, 'model': transformers.ElectraModel},
}

#@title TransformerEncoder

class TransformerEncoder(Dataset):
    def __init__(self, items_path, transformer_config='bert-base-uncased', mode="train", featurizer=None):
        super().__init__()
        assert mode=="train" or featurizer 
        self.path = items_path
        self.data = pd.read_csv(self.path, encoding = "ISO-8859-1").loc[:,"raw.title":"raw.abstract"]
        self.num_items = len(self.data)

        if featurizer:
            self.tokenizer=featurizer["tokenizer"]
            self.model=featurizer["model"]
            # self.scaler=featurizer["scaler"]
        else:
            self.dic=self.data.loc[:,['raw.title','raw.abstract']]
            self.transformer_config = transformer_config
            self.tokenizer = TRANSFORMER_PRETRAINED_MODELS[transformer_config]['tokenizer'].from_pretrained(transformer_config)


    @staticmethod
    def get_text(dic):
        titles=dic['raw.title'].values
        abstracts=dic['raw.abstract'].values
        
        text=[str(t)+". "+str(a) for (t,a) in zip(titles,abstracts)]
        return text

    def __len__(self):
        return len(self.data)
    


    def featurize_anime(self,item):


        item_feats = str(item['raw.title'])+". "+str(item['raw.abstract'])
        item_encodings = self.tokenizer(item_feats, return_tensors='pt', padding='max_length', truncation=True, max_length=512)
        input_ids = torch.squeeze(item_encodings['input_ids'])
        attention_mask = torch.squeeze(item_encodings['attention_mask'])
        return input_ids, attention_mask


    def getanime(self, idx):
        item=self.data.iloc[idx]
        return self.featurize_anime(item)

    
    def __getitem__(self, idx):
        return self.getanime(idx)


train_path="data/citeulike/article_data.csv"

rec=TransformerEncoder(train_path, transformer_config='distilbert-base-uncased')

#@title DropoutNet

class DropoutNet(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()
        assert {"item_drop_p","user_drop_p","latent_dim","hidden_dims"}.issubset(set(kwargs.keys()))
        
        for (k,v) in kwargs.items():
            setattr(self,k,v)

        self.V_dict=v_dict
        self.U_dict=u_dict
 
        self.item_drop=nn.Dropout(self.item_drop_p)
        self.user_drop=nn.Dropout(self.user_drop_p)
        #LAYERS

        
        prev_dim=self.latent_dim+self.anime_dim
        for (i,hidden_dim) in enumerate(self.hidden_dims):
            setattr(self,"hidden_u_{}".format(i),nn.Linear(prev_dim,hidden_dim))
            setattr(self,"hidden_v_{}".format(i),nn.Linear(prev_dim,hidden_dim))
            prev_dim=hidden_dim


        activation_options={"relu":nn.ReLU(),"sigmoid":nn.Sigmoid(),"tanh":nn.Tanh(),"identity":nn.Identity()}
        

        self.non_linearity=activation_options[self.activation] if "activation" in kwargs else nn.Identity()
        self.f_u=nn.Linear(self.hidden_dims[-1],self.latent_dim)
        self.f_v=nn.Linear(self.hidden_dims[-1],self.latent_dim)
        self.f_u_cache={}
        self.f_v_cache={}


    def forward(self,u,v,u_phi,v_phi,u_inds,v_inds,y,eval=False):
        #note we're using ids, not indices
        
        B=u.shape[0]
        if eval:
            u_ind_mask=np.array(list(filter(lambda i:u_inds[i] not in self.f_u_cache,range(len(u_inds)))))
            v_ind_mask=np.array(list(filter(lambda i:v_inds[i] not in self.f_v_cache,range(len(v_inds)))))
            u,u_phi,v,v_phi=u[u_ind_mask],u_phi[u_ind_mask],v[v_ind_mask],v_phi[v_ind_mask]
            u_ind_mask,v_ind_mask=u_ind_mask.astype(int),v_ind_mask.astype(int)  

        v=self.item_drop(v) #(B,h) 
        u=self.user_drop(u)
        phi_u=torch.cat((u,u_phi),axis=1)
        phi_v=torch.cat((v,v_phi),axis=1)
        for (i,hidden_dim) in enumerate(self.hidden_dims):
            phi_u=getattr(self,"hidden_u_{}".format(i))(phi_u)
            phi_v=getattr(self,"hidden_v_{}".format(i))(phi_v)
            phi_u=self.non_linearity(phi_u)
            phi_v=self.non_linearity(phi_v)

        f_u=self.f_u(phi_u)
        f_v=self.f_v(phi_v)

        if eval:
            for (u_ind,f_u_) in dict(zip(u_inds[u_ind_mask],f_u)).items():
                self.f_u_cache[u_ind]=f_u_
            for (v_ind,f_v_) in dict(zip(v_inds[v_ind_mask],f_v)).items():
                self.f_v_cache[v_ind]=f_v_
            f_u=torch.stack([self.f_u_cache[u_ind] for u_ind in u_inds])
            f_v=torch.stack([self.f_v_cache[v_ind] for v_ind in v_inds])

        f_u=f_u.view(B,1,self.latent_dim)
        f_v=f_v.view(B,self.latent_dim,1)
        out=torch.bmm(f_u,f_v)
        out=out.view(B,)
        loss=F.mse_loss(out,y)
        # pdb.set_trace()
        return out, loss

#@title TransformerNet

class TransformerNet(nn.Module):

    def __init__(self, transformer_config='bert-base-uncased', finetune=False):
        super(TransformerNet, self).__init__()
        self.transformer_config = transformer_config
        self.finetune = finetune
        # self.tokenizer = TRANSFORMER_PRETRAINED_MODELS[transformer_config]['tokenizer'].from_pretrained(transformer_config)
        self.model = TRANSFORMER_PRETRAINED_MODELS[transformer_config]['model'].from_pretrained(transformer_config, return_dict=True)
        self.model=nn.DataParallel(self.model)
        # self.model = self.model.to(device)
        if finetune:
            self.model.train()
        else:
            self.model.eval()

    def forward(self, v_input_ids, v_attention_mask, u_input_ids, u_attention_mask, num_shows):
        batch_size, max_shows, tokenize_dim = u_input_ids.size()

        v_phi = self.encode(v_input_ids, v_attention_mask)
        embedding_dim = v_phi.size(1)

        u_input_ids = u_input_ids.view(batch_size * max_shows, tokenize_dim)
        u_attention_mask = u_attention_mask.view(batch_size * max_shows, tokenize_dim)
        u_phi = self.encode(u_input_ids, u_attention_mask)
        u_phi = u_phi.view(batch_size, max_shows, 768)
        

        mask = torch.arange(max_shows)[None, :].to(num_shows.device) < num_shows[:, None]
        mask = mask.type(torch.float)
        mask = torch.repeat_interleave(mask[:, :, None], embedding_dim, dim=-1)
        
        u_phi = u_phi * mask
        u_phi = u_phi.sum(dim=1)
        u_phi = u_phi / num_shows[:, None]

        return u_phi, v_phi
    
    def encode(self, input_ids, attention_mask):
        encoding = self.model(input_ids=input_ids, attention_mask=attention_mask)
        # return encoding.last_hidden_state[:, 0, :]
        encoding = encoding.last_hidden_state[:, 0, :]
        # breakpoint()
        means = encoding.mean(dim=1, keepdim=True)
        stds = encoding.std(dim=1, keepdim=True)
        normalized_encoding = (encoding - means) / stds
        return normalized_encoding


#@title DropoutTransformerGNet
class DropoutTransformerNet(nn.Module):
    def __init__(self, transformer_config='bert-base-uncased', finetune=False, **kwargs):
        super(DropoutTransformerNet, self).__init__()
        self.encoder = TransformerNet(transformer_config, finetune)
        self.dropoutnet = DropoutNet(**kwargs)
        self.u_phi_cache={}
        self.v_phi_cache={}

    def forward(self, u, v, u_input_ids, u_attention_mask, num_shows, v_input_ids, v_attention_mask, u_ids, v_ids,y,eval=False):
        if eval:
            u_id_mask=np.array(list(filter(lambda i:u_ids[i] not in self.u_phi_cache,range(len(u_ids)))))
            u_input_ids,u_attention_mask=u_input_ids[u_id_mask],u_attention_mask[u_id_mask]
            v_id_mask=np.array(list(filter(lambda i:v_ids[i] not in self.v_phi_cache,range(len(v_ids))))) 
            v_input_ids,v_attention_mask=v_input_ids[v_id_mask],v_attention_mask[v_id_mask]
            num_shows=num_shows[u_id_mask]
            u_phi, v_phi = self.encoder(v_input_ids, v_attention_mask, u_input_ids, u_attention_mask, num_shows)
            #unstack to iterate
            u_id_mask,v_id_mask=u_id_mask.astype(int),v_id_mask.astype(int)
            for (u_id,u_phi_) in dict(zip(u_ids[u_id_mask],u_phi)).items():
                self.u_phi_cache[u_id]=u_phi_
            for (v_id,v_phi_) in dict(zip(v_ids[v_id_mask],v_phi)).items():
                self.v_phi_cache[v_id]=v_phi_ 
            u_phi=torch.stack([self.u_phi_cache[u_id] for u_id in u_ids])
            v_phi=torch.stack([self.v_phi_cache[v_id] for v_id in v_ids])
        else:    
            u_phi, v_phi = self.encoder(v_input_ids, v_attention_mask, u_input_ids, u_attention_mask, num_shows)
        
        out, loss = self.dropoutnet(u, v, u_phi, v_phi, u_ids,v_ids,y,eval)
        return out, loss

#@title Load feats

train_interact_path = "data/citeulike/dropoutnet_data/cold/train.csv"
test_interact_path = "data/citeulike/dropoutnet_data/cold/test.csv"
test_ids_path = "data/citeulike/dropoutnet_data/cold/test_item_ids.csv"
user_path="data/citeulike/dropoutnet_data/trained/cold/WRMF_cold_rank200_reg1_alpha10_iter10.U.txt"
item_path="data/citeulike/dropoutnet_data/trained/cold/WRMF_cold_rank200_reg1_alpha10_iter10.V.txt"

user_vectors = np.loadtxt(user_path)
user_vectors = user_vectors[1:, :]
item_vectors = np.loadtxt(item_path)
item_vectors = item_vectors[1:, :]

num_users = user_vectors.shape[0]
num_items = item_vectors.shape[0]
user_df = user_vectors
item_df = item_vectors

def read_iteraction_data(data_path):
    pref_matrix = np.zeros((num_users, num_items), dtype=np.int)
    user_ids = []
    item_ids = []
    for line in open(data_path):
        arr = line.strip().split(",")
        user_id = int(arr[0]) - 1
        item_id = int(arr[1]) - 1
        user_ids.append(user_id)
        item_ids.append(item_id)
        pref_matrix[user_id, item_id] = 1
    user_ids = sorted(list(set(user_ids)))
    item_ids = sorted(list(set(item_ids)))
    return user_ids, item_ids, pref_matrix

train_user_ids, train_item_ids, train_pref_matrix = read_iteraction_data(train_interact_path)
val_user_ids, val_item_ids, val_pref_matrix = read_iteraction_data(test_interact_path)

train_pref_mask=train_pref_matrix>0.0
val_pref_mask=val_pref_matrix>0.0

#redundant
all_item_ids = range(num_items)
train_item_ids_set=set(all_item_ids) - set(val_item_ids)
train_item_ids = sorted(list(train_item_ids_set))

u_dict=dict(zip(train_user_ids,user_vectors[train_user_ids]))#{user_ind:item_vec}
v_dict=dict(zip(train_item_ids,item_vectors[train_item_ids]))#{anime_ind:item_vec}

#@title get_u_feat

max_shows_per_user = int(max(np.array(train_pref_mask, dtype=np.float).sum(axis=1)))
max_shows_per_user = 5

# all_v_feats=[rec.getanime(i) for i in range(len(item_df))]
# all_v_feats_train = [all_v_feats[i] for i in train_user_ids]
# v_feats=dict(zip(train_user_ids,all_v_feats_train))

all_v_feats=[rec.getanime(i) for i in range(len(item_df))]
all_v_feats_train = [all_v_feats[i] for i in train_item_ids]
v_feats=dict(zip(train_item_ids,all_v_feats_train))

all_u_transforms=[np.average(item_vectors[train_pref_mask[i]],axis=0) for i in range(len(user_df))]
all_u_transforms_train = [all_u_transforms[i] for i in train_user_ids]
u_transforms=dict(zip(train_user_ids,all_u_transforms_train))

tokenizer = transformers.BertTokenizer.from_pretrained('distilbert-base-uncased')
empty_encodings = tokenizer("", return_tensors='pt', padding='max_length', truncation=True, max_length=512)
empty_input_ids = torch.squeeze(empty_encodings['input_ids'])
empty_attention_mask = torch.squeeze(empty_encodings['attention_mask'])

def get_u_feat(i):
    u_input_ids = []
    u_attention_mask = []
    for j, included in enumerate(train_pref_mask[i]):
        if included:
            input_ids, attention_mask = all_v_feats[j]
            u_input_ids.append(input_ids)
            u_attention_mask.append(attention_mask)
        if len(u_input_ids) >= max_shows_per_user:
            break

    num_shows_per_user = len(u_input_ids)
    if num_shows_per_user < max_shows_per_user:
        u_input_ids = u_input_ids + (max_shows_per_user - num_shows_per_user) * [empty_input_ids]
        u_attention_mask = u_attention_mask + (max_shows_per_user - num_shows_per_user) * [empty_attention_mask]
    # else:
    #     u_input_ids = u_input_ids[:max_shows_per_user]
    #     u_attention_mask = u_attention_mask[:max_shows_per_user]
    u_input_ids = torch.stack(u_input_ids)
    u_attention_mask = torch.stack(u_attention_mask)
    return u_input_ids, u_attention_mask, num_shows_per_user
    

all_u_feats=[get_u_feat(i) for i in range(len(user_df))]
all_u_feats_train = [all_u_feats[i] for i in train_user_ids]
u_feats=dict(zip(train_user_ids,all_u_feats_train))

# all_v_feats_val = [all_v_feats[i] for i in val_user_ids]
# val_v_feats=dict(zip(val_user_ids,all_v_feats_val))

all_v_feats_val = [all_v_feats[i] for i in val_item_ids]
val_v_feats=dict(zip(val_item_ids,all_v_feats_val))


#@title CiteULike Data

citeulike_data=pd.read_csv(train_interact_path,names=['user','item'],index_col=False)
citeulike_val_data=pd.read_csv(test_interact_path,names=['user','item'],index_col=False)


np.max(list(v_dict.keys()))


#@title Load cache
import json
path="cached/citeulike/"
VAR_NAMES=['u_dict','v_dict','val_v_dict']
for j in VAR_NAMES:
    with open(path+j+".json") as f:
        dic=json.load(f)
        locals()[j]={int(float(k)):np.array(dic[k]) for k in dic}

# assert len(v_dict)==8000

#@title CiteULikePosOnly

class CiteULikePosOnly(Dataset):
    def __init__(self,data,u_dict,v_dict,u_feats,v_feats):
        self.data=data
        self.V_dict=v_dict
        self.U_dict=u_dict
        self.V_feats=v_feats 
        self.U_feats=u_feats
        self.V=np.array(list(v_dict.values()))
        self.U=np.array(list(u_dict.values()))
        self.V_phi=list(v_feats.values())
        self.U_phi=list(u_feats.values())
        self.labels=np.dot(self.U,self.V.T)#(num users,num items)
        self.num_users,self.num_items=self.labels.shape
        self.user_ids=list(map(int,u_dict.keys()))
        self.item_ids=list(map(int,v_dict.keys()))#used for neg labels
        self.inv_user_ids=dict(zip(self.user_ids,range(len(self.user_ids))))
        self.inv_item_ids=dict(zip(self.item_ids,range(len(self.item_ids))))
    
    def __getitem__(self,idx): 
        user,item=self.data.iloc[idx].values-1
        u_id,v_id=user,item#already index
        u,v,u_feat,v_feat=self.U_dict[u_id],self.V_dict[v_id],self.U_feats[u_id],self.V_feats[v_id]
        y=self.labels[self.inv_user_ids[user]][self.inv_item_ids[item]]
        return (u,v) + u_feat + v_feat + (y,)

    def __len__(self): 
        return len(self.data)

#@title CiteULikeNegSample

import random
import pdb
class CiteULikeNegSample(CiteULikePosOnly):
    def __init__(self,data,u_dict,v_dict,u_feats,v_feats,neg_thresh=-10,pos_neg_ratio=1.0,row_major=True):
        
        super().__init__(data,u_dict,v_dict,u_feats,v_feats)
        flat_labels=self.labels.reshape((-1,))
        neg_mask=flat_labels<neg_thresh
        print("Neg:",np.sum(neg_mask))
        self.neg_inds=np.arange(len(flat_labels))[neg_mask]
        num_neg=int(len(self.data)*pos_neg_ratio)
        assert num_neg<len(self.neg_inds)
        self.sampled_neg_inds=np.random.choice(self.neg_inds,(num_neg,),replace=False)

        self.row_major=row_major
        self.major_len=self.num_users if row_major else self.num_items
        self.minor_len=self.num_items if row_major else self.num_users
        

    def __getitem__(self,idx):
        #this first exhausts pos samples from super().__getitem__() before 
        #going to negative
        if idx<len(self.data):
            return super().__getitem__(idx)
        else:
            idx=self.sampled_neg_inds[idx-len(self.data)]
    
            major_ind=idx%self.major_len
            minor_ind=random.randrange(self.minor_len)
            inds=[minor_ind,major_ind]
            if self.row_major: inds=inds[::-1]
            user,item=inds
            u_id,v_id=self.user_ids[user],self.item_ids[item]
            u,v,u_feat,v_feat=self.U_dict[u_id],self.V_dict[v_id],self.U_feats[u_id],self.V_feats[v_id]
            y=self.labels[user][item]
            return (u,v) + u_feat + v_feat + (y,)
            
      
    def __len__(self): 
        return len(self.sampled_neg_inds)+len(self.data)

#@title EDA


#@title Initialize model
import torch.optim as optim
from torch.optim.lr_scheduler import MultiplicativeLR,LambdaLR
from sklearn.model_selection import ParameterGrid as pg


class MyDataParallel(nn.DataParallel):
    def __getattr__(self, name):
        return getattr(self.module, name)

config={
    "hidden_dims":[[800,400]],
    "latent_dim":[200],#FIX
    "anime_dim":[768],#FIX
    "item_drop_p":[0.5],
    "user_drop_p":[0.5],
    "activation":["tanh"]
}

configs=pg(config)
#@title BERT params 
fine_tune_param_names={ 'encoder.model.transformer.layer.5.attention.q_lin.weight',
 'encoder.model.transformer.layer.5.attention.q_lin.bias',
 'encoder.model.transformer.layer.5.attention.k_lin.weight',
 'encoder.model.transformer.layer.5.attention.k_lin.bias',
 'encoder.model.transformer.layer.5.attention.v_lin.weight',
 'encoder.model.transformer.layer.5.attention.v_lin.bias',
 'encoder.model.transformer.layer.5.attention.out_lin.weight',
 'encoder.model.transformer.layer.5.attention.out_lin.bias',
 'encoder.model.transformer.layer.5.sa_layer_norm.weight',
 'encoder.model.transformer.layer.5.sa_layer_norm.bias',
 'encoder.model.transformer.layer.5.ffn.lin1.weight',
 'encoder.model.transformer.layer.5.ffn.lin1.bias',
 'encoder.model.transformer.layer.5.ffn.lin2.weight',
 'encoder.model.transformer.layer.5.ffn.lin2.bias',
 'encoder.model.transformer.layer.5.output_layer_norm.weight',
 'encoder.model.transformer.layer.5.output_layer_norm.bias',
 'dropoutnet.linear_u.weight',
 'dropoutnet.linear_u.bias',
 'dropoutnet.linear_v.weight',
 'dropoutnet.linear_v.bias',
 'dropoutnet.linear_phi_u.weight',
 'dropoutnet.linear_phi_u.bias',
 'dropoutnet.linear_phi_v.weight',
 'dropoutnet.linear_phi_v.bias',
 'dropoutnet.f_u.weight',
 'dropoutnet.f_u.bias',
 'dropoutnet.f_v.weight',
 'dropoutnet.f_v.bias'
}
fine_tune_bert_param_names={ 'encoder.model.transformer.layer.5.attention.q_lin.weight',
 'encoder.model.transformer.layer.5.attention.q_lin.bias',
 'encoder.model.transformer.layer.5.attention.k_lin.weight',
 'encoder.model.transformer.layer.5.attention.k_lin.bias',
 'encoder.model.transformer.layer.5.attention.v_lin.weight',
 'encoder.model.transformer.layer.5.attention.v_lin.bias',
 'encoder.model.transformer.layer.5.attention.out_lin.weight',
 'encoder.model.transformer.layer.5.attention.out_lin.bias',
 'encoder.model.transformer.layer.5.sa_layer_norm.weight',
 'encoder.model.transformer.layer.5.sa_layer_norm.bias',
 'encoder.model.transformer.layer.5.ffn.lin1.weight',
 'encoder.model.transformer.layer.5.ffn.lin1.bias',
 'encoder.model.transformer.layer.5.ffn.lin2.weight',
 'encoder.model.transformer.layer.5.ffn.lin2.bias',
 'encoder.model.transformer.layer.5.output_layer_norm.weight',
 'encoder.model.transformer.layer.5.output_layer_norm.bias'
}
fine_tune_dropoutnet_param_names={
 'dropoutnet.linear_u.weight',
 'dropoutnet.linear_u.bias',
 'dropoutnet.linear_v.weight',
 'dropoutnet.linear_v.bias',
 'dropoutnet.linear_phi_u.weight',
 'dropoutnet.linear_phi_u.bias',
 'dropoutnet.linear_phi_v.weight',
 'dropoutnet.linear_phi_v.bias',
 'dropoutnet.f_u.weight',
 'dropoutnet.f_u.bias',
 'dropoutnet.f_v.weight',
 'dropoutnet.f_v.bias'
}
def get_optimizer(dn):
    fine_tune_params=list(map(lambda x:x[1],filter(lambda x:x[0] in fine_tune_param_names,dn.named_parameters())))
    regular_params=list(map(lambda x:x[1],filter(lambda x:x[0] not in fine_tune_param_names,dn.named_parameters())))
    for regular_param in regular_params:
        regular_param.requires_grad = False
    fine_tune_bert_params=list(map(lambda x:x[1],filter(lambda x:x[0] in fine_tune_bert_param_names,dn.named_parameters())))
    fine_tune_dropoutnet_params=list(map(lambda x:x[1],filter(lambda x:x[0] in fine_tune_dropoutnet_param_names,dn.named_parameters())))
    optimizer=optim.Adam([{'params': fine_tune_dropoutnet_params},
                          {'params': fine_tune_bert_params, 'lr': 1e-4/8}
                        ], lr=1e-4/8)
    return optimizer

#@title Learning rate adjust
def adjust_learning_rate(optimizer, epoch,rate=0.9):
    """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
    lr = (1e-4/8) * (rate ** epoch)

#@title Training

from tqdm import tqdm
def evaluate(epoch,dn,val_dload,preds=None,tru=None):
    dn.eval()
    with torch.no_grad():
        eval_loss=0.0
        for (j,data) in tqdm(enumerate(val_dload,0)):

            u, v, u_input_ids, u_attention_mask, num_shows, v_input_ids, v_attention_mask, u_ids,v_ids, y = data
                

            v=np.zeros_like(v)
            u, v = torch.tensor(u,dtype=torch.float).to(device), torch.tensor(v,dtype=torch.float).to(device)
            u_input_ids, u_attention_mask, num_shows = u_input_ids.to(device), u_attention_mask.to(device), num_shows.to(device)
            # print(u_scalars.dtype)
            v_input_ids, v_attention_mask = v_input_ids.to(device), v_attention_mask.to(device)
            
            y = torch.tensor(y,dtype=torch.float).to(device)
            u_ids,v_ids=u_ids.numpy(),v_ids.numpy()
            r,loss=dn(u, v, u_input_ids, u_attention_mask, num_shows, v_input_ids, v_attention_mask, u_ids,v_ids,y,True)
            if preds!=None and tru!=None:
                preds.extend(r.cpu().numpy())
                tru.extend(y.cpu().numpy())
            loss=loss.mean()
            eval_loss+=loss.item()
            
        print('[%d] eval loss: %.5f' %(epoch + 1,  eval_loss / (j+1)))
        return eval_loss / (j+1)
            
#@title UserAnimeID

class UserAnimeID(Dataset):
    def __init__(self,u_dict,v_dict,u_feats,v_feats):
        self.V_dict=v_dict
        self.U_dict=u_dict
        self.V_feats=v_feats 
        self.U_feats=u_feats
        self.V=np.array(list(v_dict.values()))
        self.U=np.array(list(u_dict.values()))
        self.V_phi=list(v_feats.values())
        self.U_phi=list(u_feats.values())
        self.labels=np.dot(self.U,self.V.T)#(num users,num items)
        self.num_users,self.num_items=self.labels.shape
        self.user_ids=list(map(int,u_dict.keys()))
        self.item_ids=list(map(int,v_dict.keys()))
    
    def __getitem__(self,idx): 
        item=idx%self.num_items 
        user=(idx-item)//self.num_items
        # u_id,v_id=self.user_item_pairs[idx]
        u_id,v_id=self.user_ids[user],self.item_ids[item]
        u,v,u_phi,v_phi=self.U_dict[u_id],self.V_dict[v_id],self.U_feats[u_id],self.V_feats[v_id]
        y=self.labels[user][item]
        return (u,v) + u_phi + v_phi + (u_id,v_id,)+(y,)

    def __len__(self): 
      return self.num_items * self.num_users


#@title UserAnimeCacheID

#now we only care about seeing every user and item at least once

class UserAnimeCacheID(UserAnimeID):
    def __init__(self,u_dict,v_dict,u_feats,v_feats):
        super().__init__(u_dict,v_dict,u_feats,v_feats)
        self.perimeter_idxes=list(range(len(v_dict)))+(len(v_dict)*np.arange(1,len(u_dict))).tolist()
    def __getitem__(self, idx):
        return super().__getitem__(self.perimeter_idxes[idx])
    def __len__(self):
        return len(self.perimeter_idxes)

#@title SlicedUserAnimeID

class SlicedUserAnimeID(UserAnimeID):
    def __init__(self,u_dict,v_dict,u_feats,v_feats,slice_users=10,slice_items=0):
        super().__init__(u_dict,v_dict,u_feats,v_feats)
        assert slice_users ^ slice_items
        self.slice_users=(slice_users>0)
        self.sliced_users=slice_users
        self.sliced_items=slice_items
        
    
    def __getitem__(self,idx): 
        prog=idx%(self.num_items if self.slice_users else self.num_users)
        dom=self.num_items if self.slice_users else self.num_users
        passes=(idx-prog)//dom
        #swap axis
        idx=passes*(self.num_items if self.slice_users else self.num_users)+prog
        return super().__getitem__(idx)

        
    def __len__(self): 
        return self.num_items*self.sliced_users if self.slice_users else self.num_users*self.sliced_items

#@title Eval
import heapq



def item_recall_at_M(M, R_hat_np, val_pref_matrix):
    ranking = np.argsort(-R_hat_np, axis=1)
    topM = ranking[:,:M]

    n = np.array([len(set(a) & set(np.nonzero(b)[0])) for a, b in zip(topM, val_pref_matrix)])
    d = np.sum(val_pref_matrix> 0, axis=1)
    
    return np.mean(n[d>0] / d[d>0])

def user_recall_at_M(M, R_hat_np, val_pref_matrix):
    ranking = np.argsort(-R_hat_np, axis=1)
    topM = ranking[:,:M]

    pref = val_pref_matrix

    n = np.array([np.count_nonzero(topM[pref[:,i] > 0] == i) for i in range(pref.shape[1])])
    d = np.sum(pref > 0, axis=0)
    
    return np.mean(n[d>0] / d[d>0])

def cold_metric(n_users,n_items,config=None,dn=None):
    assert min(n_users,n_items)==0
    cache_uaid=UserAnimeCacheID(u_dict,val_v_dict,u_feats,val_v_feats)
    # sliced_uaid=SlicedUserAnimeID(u_dict,v_dict,u_feats,v_feats,slice_users=n_users,slice_items=n_items)
    dload=DataLoader(cache_uaid,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS)
    preds,tru=[],[]
    if not dn:
        assert config
        assert "PATH" in globals()
        dn=DropoutTransformerNet(**config)
        dn.load_state_dict(torch.load(PATH,map_location=device))
        dn=dn.to(device)
    evaluate(0,dn,dload)
    print("CACHED")
    preds,tru=[],[]
    U_hat=torch.stack([dn.dropoutnet.f_u_cache[k] for k in u_dict])
    V_hat=torch.stack([dn.dropoutnet.f_v_cache[k] for k in val_v_dict])
    R_hat=U_hat.matmul(V_hat.T)
    R_hat_np=R_hat.cpu().numpy()
    ir=item_recall_at_M(100, R_hat_np, val_pref_matrix)
    ur=user_recall_at_M(100, R_hat_np, val_pref_matrix)
    return (ir,ur)

metrics=cold_metric(0,len(val_v_dict),list(pg(config))[0])
print(metrics)






