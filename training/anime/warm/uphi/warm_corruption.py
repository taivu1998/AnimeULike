# -*- coding: utf-8 -*-
"""DEEPANIGNET.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D_aoK_b4TMTh1hQ63fNIr0433bH8WKsU
"""

import os


os.environ['HOME']="dfs/user/msun415"

import argparse
parser = argparse.ArgumentParser()

parser.add_argument('-d', action='store',
                    dest='devices',type=str,
                    help='devices ids, comma sep')
parser.add_argument('-b', action='store',
                    dest='batch_size',
                    help='batch size')       
parser.add_argument('-n', action='store',
                    dest='num_workers',
                    help='num workers')                          

parser.add_argument('-a', action='store',
                    dest='num_accumulation_steps',
                    help='number of accumulation steps') 
 


parser.add_argument('-e', action='store',
                    dest='metric_every',
                    help='how many batches per computing of metric')    
                    


results = parser.parse_args()
os.environ['CUDA_VISIBLE_DEVICES']=results.devices
device = "cuda:0"
BATCH_SIZE,NUM_WORKERS=int(results.batch_size),int(results.num_workers)
NUM_EPOCHS=10
NUM_ACCUMULATION_STEPS=int(results.num_accumulation_steps)
METRIC_EVERY=int(results.metric_every)
assert BATCH_SIZE*NUM_ACCUMULATION_STEPS==100, "to be consistent with DropoutNet"
# assert METRIC_EVERY*BATCH_SIZE==5000, "to be consistent"

LR_DECAY_RATE=1.0
LR_SCALE_FACTOR=BATCH_SIZE*NUM_ACCUMULATION_STEPS/100
MOMENTUM=0.0

import pandas as pd
from collections import OrderedDict
from torch.utils.data import Dataset

import numpy as np
from abc import abstractmethod
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn import preprocessing as prep
from typing import List
import torch
from torch.nn import functional as F

import transformers
import torch.nn as nn
import itertools
from torch.utils.data import DataLoader


import torch
import networkx as nx
import matplotlib.pyplot as plt


def visualize(h, color, epoch=None, loss=None):
    plt.figure(figsize=(7,7))
    plt.xticks([])
    plt.yticks([])

    if torch.is_tensor(h):
        h = h.detach().cpu().numpy()
        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap="Set2")
        if epoch is not None and loss is not None:
            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)
    else:
        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,
                         node_color=color, cmap="Set2")
    plt.show()

print("Imported libraries")
# Commented out IPython magic to ensure Python compatibility.

# %cd 'drive/MyDrive/CS224N Project'

#@title RecommendationsDataset
from collections import OrderedDict as od
class RecommendationDataset(Dataset):
    def __init__(self, index_path, animes_path, recommendations_path):
        self.neg_k=3
        
        self.rc_path=recommendations_path
        self.rc_data=pd.read_csv(self.rc_path)
        gby_show=self.rc_data.groupby(['show1'])
        gby_shows=self.rc_data.groupby(['show1','show2'])
        self.gby_show={k:np.array(v) for (k,v) in gby_show.groups.items()}
        self.gby_shows={k:np.array(v) for (k,v) in gby_shows.groups.items()}
        self.path=animes_path

        #pre-processing
        

        self.data=pd.read_csv(self.path).loc[:,"synopsis":"reviews"]
        self.num_anime=len(self.data)
        indices=list(map(int,open(index_path,'r').readlines()))
        index_range=range(len(indices))
        self.index_map=dict(zip(indices,index_range))
        self.map_index=dict(zip(index_range,indices))
        self.scalar_inds=["rating","n1","n2","n3","n4","n5"]
        self.text_inds=["synopsis","reviews"]
        scaler=MinMaxScaler()
        vals=self.data.loc[:,self.scalar_inds].values
        scaler.fit(vals)
        self.scaler=scaler
        self.numerics=self.scaler.transform(vals)
    

    def __len__(self):
        return len(self.rc_data)

    def __negsample__(self,idx):
        show1=self.rc_data.iloc[idx,0]
        idxes2=np.random.choice(self.gby[idx],self.neg_k,False)
        neg_shows=self.rc_data.iloc[idxes2,1]
        neg_show_inds=[self.index_map[s] for s in neg_shows]
        neg_mask=np.ones(self.num_anime)
        neg_mask[neg_show_inds]=0
        neg_indices=np.arange(self.num_anime)[neg_mask]
        neg_shows=list(map(lambda x:self.map_index[x],neg_indices))
        return neg_shows


    def __getitem__(self, idx):
        id_1=int(self.rc_data["show1"][idx])
        id_2=int(self.rc_data["show2"][idx])
        data_1=self.data.iloc[self.index_map[id_1]].values
        data_2=self.data.iloc[self.index_map[id_2]].values
        return np.concatenate((data_1,data_2))


from functools import reduce

class TFIDF(RecommendationDataset):
    def __init__(self,index_path,train_lookup,animes_path,recs_path,max_features=2000,mode="train",featurizer=None):
        super().__init__(index_path,train_lookup,animes_path,recs_path)
        print(super().__len__())
        #featurizer={"fzer":TfidfFeaturizer,"scaler":MinMaxScaler}
        assert mode=="train" or featurizer 
        if featurizer:
            self.fzer=featurizer["fzer"]
            self.scaler=featurizer["scaler"]
        else:
            self.tf_config={
                'strip_accents':'ascii',
                'stop_words':'english',
                'max_features':max_features
            }
            self.dic=self.data.loc[:,self.text_inds]
            self.fzer=self.fit_text(TFIDF.get_text(self.dic))
            self.scalar_dic=self.data.loc[:,self.scalar_inds]
            self.scaler=self.fit_scalars(self.scalar_dic)

    def top_words(self):
        return self.fzer.vocabulary_

    @staticmethod
    def get_text(dic,text_inds=["synopsis","reviews","recs"]):
        args=[dic[key].values for key in text_inds]
        text=[reduce(lambda cum,cur:str(cum)+str(cur),x) for x in zip(*args)]
        return text

    def __len__(self):
        return len(self.data)

    def fit_text(self,text:List[str]):
        fzer=TfidfVectorizer(**self.tf_config)
        fzer.fit(text)
        return fzer
    
    def fit_scalars(self,scalars:List[float]):
        scaler=MinMaxScaler()
        scaler.fit(scalars)
        return scaler


    def featurize_anime(self,item):
        feats=self.featurize([str(item.synopsis)+str(item.reviews)])
        scalars=self.scale([item.loc[self.scalar_inds]])
        x=np.concatenate((scalars[0],feats[0].toarray()[0]))
        return x.astype(float)
    
    def featurize(self, text: List[str]):
        #assume fzer already transformed on train
        return self.fzer.transform(text)
    
    def scale(self, scalars):
        #same for scaler
        return self.scaler.transform(scalars)

    def getanime(self, idx):
        item=self.data.iloc[idx]
        return self.featurize_anime(item)

    def __getitem__(self, idx):
        id_1=int(self.rc_data["show1"][idx])
        id_2=int(self.rc_data["show2"][idx])
        idx1=self.index_map[id_1]
        idx2=self.index_map[id_2]
        x=np.concatenate((self.getanime(idx1),self.getanime(idx2)))
        return torch.tensor(x.astype(float),dtype=torch.float)

#@title Pretrained models
TRANSFORMER_PRETRAINED_MODELS = {
    'bert-base-uncased': {'tokenizer': transformers.BertTokenizerFast, 'model': transformers.BertModel},
    'pretraining/bert-anime2vec':{'tokenizer': transformers.BertTokenizerFast, 'model': transformers.BertModel},
    'distilbert-base-uncased': {'tokenizer': transformers.DistilBertTokenizerFast, 'model': transformers.DistilBertModel},
    'roberta-base': {'tokenizer': transformers.RobertaTokenizer, 'model': transformers.RobertaModel},
    'google/electra-small-discriminator': {'tokenizer': transformers.ElectraTokenizer, 'model': transformers.ElectraModel},
    'google/electra-small-discriminator': {'tokenizer': transformers.ElectraTokenizer, 'model': transformers.ElectraModel},
    'pretraining/distillbert-anime2vec': {'tokenizer': transformers.DistilBertTokenizerFast, 'model': transformers.DistilBertModel},
    'pretraining/distillbert-anime2vec-2': {'tokenizer': transformers.DistilBertTokenizerFast, 'model': transformers.DistilBertModel},
    'pretraining/distillbert-anime2vec-3': {'tokenizer': transformers.DistilBertTokenizerFast, 'model': transformers.DistilBertModel},
    'pretraining/distillbert-anime2vec-4': {'tokenizer': transformers.DistilBertTokenizerFast, 'model': transformers.DistilBertModel},
}

#@title TransformerFinetuneDataset

class TransformerFinetuneDataset(RecommendationDataset):
    def __init__(self, index_path, animes_path, recs_path, transformer_config='bert-base-uncased', mode="train",featurizer=None):
        super().__init__(index_path,animes_path,recs_path)
        assert mode=="train" or featurizer 
        

        if featurizer:
            self.tokenizer=featurizer["tokenizer"]
            self.scaler=featurizer["scaler"]
        else:
            self.dic=self.data.loc[:,self.text_inds]
            self.transformer_config = transformer_config
            # self.device = device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.tokenizer = TRANSFORMER_PRETRAINED_MODELS[transformer_config]['tokenizer'].from_pretrained(transformer_config)
            self.scalar_dic=self.data.loc[:,self.scalar_inds]
            self.scaler=self.fit_scalars(self.scalar_dic)

    @staticmethod
    def get_text(dic):
        synopses=dic.synopsis.values
        reviews=dic.reviews.values
        
        text=[str(s)+str(r) for (s,r) in zip(synopses,reviews)]
        return text

    def __len__(self):
        return len(self.data)
    
    def fit_scalars(self,scalars:List[float]):
        scaler=MinMaxScaler()
        scaler.fit(scalars)
        return scaler

    def featurize_anime(self,item):
        item_feats = str(item.synopsis)+str(item.reviews)
        item_encodings = self.tokenizer(item_feats, return_tensors='pt', padding='max_length', truncation=True, max_length=512)
        input_ids = torch.squeeze(item_encodings['input_ids'])
        attention_mask = torch.squeeze(item_encodings['attention_mask'])
        scalars=self.scale([item.loc[self.scalar_inds]])
        scalars=torch.squeeze(torch.tensor(scalars,dtype=torch.float))
        return input_ids, attention_mask, scalars
    
    def scale(self, scalars):
        #same for scaler
        return self.scaler.transform(scalars)

    def getanime(self, idx):
        item=self.data.iloc[idx]
        return self.featurize_anime(item)

    def __getitem__(self, idx):
        id_1=int(self.rc_data["show1"][idx])
        id_2=int(self.rc_data["show2"][idx])
        idx1=self.index_map[id_1]
        idx2=self.index_map[id_2]
        return self.getanime(idx1) + self.getanime(idx2)

ANIME_PATH="data/anime/train_val_test/10000"

train_path="{}/10000_animes_warm.csv".format(ANIME_PATH)
recs_path="{}/10000_recs.csv".format(ANIME_PATH)
lookup_path="{}/10000_shows_to_process.txt".format(ANIME_PATH)
train_lookup="{}/10000_warm.txt".format(ANIME_PATH)
rec=TransformerFinetuneDataset(lookup_path,train_path,recs_path, transformer_config='pretraining/bert-anime2vec')

valid_rec_indices=[e[0] in rec.index_map and e[1] in rec.index_map for e in zip(rec.rc_data.show1,rec.rc_data.show2)] 

import pdb


#@title DropoutGNet


class DropoutGNet(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()
        for (k,v) in kwargs.items():
            setattr(self,k,v)

        self.V_dict=v_dict
        self.U_dict=u_dict 

        self.edge_attr=len(self.data_mode)==2
        
        self.data=data_inits(*(self.data_mode if self.edge_attr else self.data_mode + ("tf-idf",)))
        self.node_dim=self.data.x.size(-1)

        #LAYERS
        prev_dim=self.latent_dim+self.anime_dim+self.num_gnn_feats
        for (i,hidden_dim) in enumerate(self.hidden_dims):            
            hidden_v=nn.Linear(prev_dim,hidden_dim)
            nn.init.normal_(hidden_v.weight,std=0.01)
            nn.init.zeros_(hidden_v.bias)
            hidden_u=nn.Linear(prev_dim if i else prev_dim-self.num_gnn_feats,hidden_dim)
            nn.init.normal_(hidden_u.weight,std=0.01)
            nn.init.zeros_(hidden_u.bias)
            setattr(self,"hidden_u_{}".format(i),nn.DataParallel(hidden_u))
            setattr(self,"hidden_v_{}".format(i),nn.DataParallel(hidden_v))        
            setattr(self,"hidden_u_{}_norm".format(i),nn.DataParallel(nn.BatchNorm1d(hidden_dim)))
            setattr(self,"hidden_v_{}_norm".format(i),nn.DataParallel(nn.BatchNorm1d(hidden_dim)))            
            prev_dim=hidden_dim
        
        activation_options={"relu":nn.ReLU(),"sigmoid":nn.Sigmoid(),"tanh":nn.Tanh(),"identity":nn.Identity()}
        self.non_linearity=activation_options[self.activation] if "activation" in kwargs else nn.Identity()
        f_u=nn.Linear(self.hidden_dims[-1],self.latent_dim)
        nn.init.normal_(f_u.weight,std=0.01)
        nn.init.zeros_(f_u.bias)
        self.f_u=nn.DataParallel(f_u)        
        f_v=nn.Linear(self.hidden_dims[-1],self.latent_dim)
        nn.init.normal_(f_v.weight,std=0.01)
        nn.init.zeros_(f_v.bias)
        self.f_v=nn.DataParallel(f_v)



        if not self.num_gnn_feats: self.num_gnn_feats=1
        if self.edge_attr:
            self.edge_feats=self.data.edge_attr
            
            edge_embed=nn.Linear(self.edge_feats.size(-1),self.node_dim)

            l1=nn.Linear(self.data.num_features,3*self.num_gnn_feats)
            l2=nn.Linear(3*self.num_gnn_feats,2*self.num_gnn_feats)
            l3=nn.Linear(2*self.num_gnn_feats,self.num_gnn_feats)

            # nn.init.normal_(edge_embed.weight,std=0.01)
            # nn.init.zeros_(edge_embed.bias)
            # nn.init.normal_(l1.weight,std=0.01)
            # nn.init.zeros_(l1.bias)
            # nn.init.normal_(l2.weight,std=0.01)
            # nn.init.zeros_(l2.bias)
            # nn.init.normal_(l3.weight,std=0.01)
            # nn.init.zeros_(l3.bias)

            self.edge_embed=edge_embed

            self.gine=GINEConv(nn.Sequential(OrderedDict([                ('linear1', l1),                ('tanh1', nn.Tanh()),                ('linear2', l2),                ('tanh2', nn.Tanh()),                ('linear3', l3),                ('tanh3', nn.Tanh())              ])            ))
        else:
            
            self.conv1=GCNConv(self.data.num_features, 3*self.num_gnn_feats)
            self.conv2=GCNConv(3*self.num_gnn_feats,2*self.num_gnn_feats)
            self.conv3=GCNConv(2*self.num_gnn_feats,self.num_gnn_feats)

        self.f_u_cache={}
        self.f_v_cache={}
        if self.num_gnn_feats==1: self.num_gnn_feats=0

    def mask_edge_inds(self,x,v_inds):
        bool_mask1=self.data.edge_index[0,:].repeat((v_inds.shape[0],1))==v_inds.reshape(-1,1)
        bool_mask2=self.data.edge_index[1,:].repeat((v_inds.shape[0],1))==v_inds.reshape(-1,1)
        bool_mask1=torch.sum(bool_mask1,axis=0)>0
        bool_mask2=torch.sum(bool_mask2,axis=0)>0
        bool_mask=torch.logical_and(bool_mask1,bool_mask2)
        masked_inds=torch.squeeze(torch.nonzero(bool_mask),dim=-1)
        return masked_inds
    
    def update(self,v_inds):
        x=self.data.x.to(device)
        
        masked_inds=self.mask_edge_inds(x,v_inds)
        edge_inds=self.data.edge_index[:,masked_inds].to(device)#(2,num rel edges)
        
        if self.edge_attr:
            #gineconv
            edge_feats=self.edge_feats[masked_inds,:].float().to(device)
            edge_attrs=self.edge_embed(edge_feats)          

            x=self.gine(x,edge_index=edge_inds,edge_attr=edge_attrs)
        else:
            x=self.conv1(x,edge_inds)
            x=x.tanh()
            x=self.conv2(x,edge_inds)
            x=x.tanh()
            x=self.conv3(x,edge_inds)
            x=x.tanh()

        
        self.node_reprs=(x-x.mean(axis=0))/x.std(axis=0)

    def forward(self,u,v,u_phi,v_phi,u_inds,v_inds,y,eval=False):
        B=u.shape[0]
        if eval:
            u_ind_mask=np.array(list(filter(lambda i:u_inds[i] not in self.f_u_cache,range(len(u_inds)))))
            v_ind_mask=np.array(list(filter(lambda i:v_inds[i] not in self.f_v_cache,range(len(v_inds)))))            
            u,u_phi,v,v_phi=u[u_ind_mask],u_phi[u_ind_mask],v[v_ind_mask],v_phi[v_ind_mask]
            u_ind_mask,v_ind_mask=u_ind_mask.astype(int),v_ind_mask.astype(int)
            nodes=self.node_reprs
            nodes=nodes[v_inds[v_ind_mask]]


        else:
            nodes=self.node_reprs
            nodes=nodes[v_inds]

        v_phi=(torch.cat((v_phi,nodes),axis=1) if self.num_gnn_feats else v_phi) if self.anime_dim else nodes
            
        
        if eval:
            v_mask=torch.rand_like(v)<self.eip 
        else:
            v_mask=torch.rand_like(v)<self.item_corrupt_p
            v_mask=(torch.randn(*v.shape)*STD+MEAN).to(device)*v_mask
        
            
        v=v_mask*v if eval else v_mask+v

        u_mask=torch.rand_like(u)<(self.eup if eval else self.user_drop_p)
        u=u_mask*u

        phi_u=torch.cat((u,u_phi),axis=1)
        phi_v=torch.cat((v,v_phi),axis=1)

        for (i,hidden_dim) in enumerate(self.hidden_dims):
            phi_u=getattr(self,"hidden_u_{}".format(i))(phi_u)
            phi_v=getattr(self,"hidden_v_{}".format(i))(phi_v)
            phi_u=getattr(self,"hidden_u_{}_norm".format(i))(phi_u)
            phi_v=getattr(self,"hidden_v_{}_norm".format(i))(phi_v)
            phi_u=self.non_linearity(phi_u)
            phi_v=self.non_linearity(phi_v)

        f_u=self.f_u(phi_u)
        f_v=self.f_v(phi_v)

        if eval:
            for (u_ind,f_u_) in dict(zip(u_inds[u_ind_mask],f_u)).items():
                self.f_u_cache[u_ind]=f_u_
            for (v_ind,f_v_) in dict(zip(v_inds[v_ind_mask],f_v)).items():
                self.f_v_cache[v_ind]=f_v_
            
            f_u=torch.stack([self.f_u_cache[u_ind] for u_ind in u_inds])            
            f_v=torch.stack([self.f_v_cache[v_ind] for v_ind in v_inds])

        f_u=f_u.view(B,1,self.latent_dim)
        f_v=f_v.view(B,self.latent_dim,1)
        out=torch.bmm(f_u,f_v)
        out=out.view(B,)
        loss=F.mse_loss(out[out==out],y[out==out])
        if loss!=loss:
            pdb.set_trace()
        return out, loss



#@title Load feats

print("Loading data")

user_path="{}/10000_warm_user_factors_large.csv".format(ANIME_PATH)
user_df=pd.read_csv(user_path)

train_ids="{}/10000_warm.txt".format(ANIME_PATH)



train_ids=list(map(lambda x:rec.index_map[int(x)],open(train_ids).readlines()))#inds, not ids

user_ids=user_df.iloc[:,0]#inds

pref_matrix,val_pref_matrix=pd.read_csv("{}/10000_warm_pref_matrix_train_0.csv".format(ANIME_PATH),index_col=0),pd.read_csv("{}/10000_warm_pref_matrix_test_0.csv".format(ANIME_PATH),index_col=0)
pref_mask=pref_matrix>0.0
val_pref_mask=val_pref_matrix>0.0
warm_user_factors=pd.read_csv("{}/10000_warm_user_factors_large.csv".format(ANIME_PATH),index_col=0)
warm_item_factors=pd.read_csv("{}/10000_warm_item_factors_large.csv".format(ANIME_PATH),index_col=0)


u,v=warm_user_factors.values,warm_item_factors.values
def prep_standardize_dense(x):
    scaler = prep.StandardScaler().fit(x)
    x_scaled = scaler.transform(x)
    x_scaled[x_scaled > 5] = 5
    x_scaled[x_scaled < -5] = -5
    x_scaled[np.absolute(x_scaled) < 1e-5] = 0
    return scaler, x_scaled


_,u=prep_standardize_dense(u)
_,v=prep_standardize_dense(v)
u_dict=dict(zip(user_ids,u))#{user_ind:item_vec}
v_dict=dict(zip(train_ids,v))#{anime_ind:item_vec}

MEAN,STD=v.mean(),v.std()

print("Loaded data")


#@title get_u_feat

#@title Load graph data
from sklearn.preprocessing import OneHotEncoder
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, GINEConv
import scipy
from scipy import stats

print("Loading graph data")


enc=OneHotEncoder()
one_hot_anime_feats=np.arange(10000)
one_hot_anime_feats=enc.fit_transform(one_hot_anime_feats.reshape(-1,1)).toarray()
tfidf_anime_feats=np.load("cached/recsys/tfidf_anime_feats.npy",allow_pickle=False)
edge_inds=np.load("cached/recsys/edge_inds.npy",allow_pickle=False)
tfidf_edge_feats=np.load("cached/recsys/tfidf_edge_feats.npy",allow_pickle=False)
make_data=lambda a_feats,e_feats:Data(torch.as_tensor(a_feats,dtype=torch.float32),
                                   torch.as_tensor(edge_inds.T,dtype=torch.long),
                                   torch.as_tensor(e_feats))

print("Loaded graph data")

print("Computing feats")


#@title Load cache
import json
path="cached/anime2vec/bert-pretrained/"
VAR_NAMES=['v_feats','val_v_feats','u_feats']
for j in VAR_NAMES:
    with open(path+j+".json") as f:
        dic=json.load(f)
        locals()[j]={int(float(k)):np.array(dic[k]) for k in dic}





v_feats_,val_v_feats_=np.array(list(v_feats.values())),np.array(list(val_v_feats.values()))
all_v_feats=prep_standardize_dense(np.append(v_feats_,val_v_feats_,axis=0))[1]
v_feats=dict(zip(train_ids,all_v_feats))
v_np=np.array(list(v_feats.values()))
u_fe=[np.average(v_np[np.argwhere(pref_matrix.iloc[i].values>0).flatten()],axis=0) for i in range(pref_matrix.shape[0])]
u_feats=dict(zip(u_feats.keys(),prep_standardize_dense(np.array(u_fe))[1]))


    
class UserAnimePermuteID(Dataset):
    def __init__(self,u_dict,v_dict,u_feats,v_feats,n_scores_user=25,pos_neg_ratio=5,user_transform_p=0.5):
        self.n_scores_user=n_scores_user
        self.V_dict=v_dict
        self.U_dict=u_dict
        self.V_feats=v_feats 
        self.U_feats=u_feats
        self.V=np.array(list(v_dict.values()))
        self.U=np.array(list(u_dict.values()))
        self.V_phi=np.array(list(v_feats.values()))
        self.U_phi=np.array(list(u_feats.values()))
        raw_labels=np.dot(self.U,self.V.T)#(num users,num items)
        # self.labels=(raw_labels- np.mean(raw_labels)) / np.std(raw_labels)
        self.labels=raw_labels
        self.num_users,self.num_items=self.labels.shape
        self.user_ids=list(map(int,u_dict.keys()))
        self.item_ids=list(map(int,v_dict.keys()))
        self.user_permute=np.random.permutation(self.num_users)

        neg_random=lambda num: np.random.permutation(self.item_ids)[:num]
        num_neg=int(n_scores_user*pos_neg_ratio/(pos_neg_ratio+1))
        leftover_pos=n_scores_user-num_neg
        pmatrix=pref_matrix.values
        def pos_random(i):
            vals=np.array(self.item_ids)[np.argwhere(pmatrix[i]>0)[:,0]]
            if len(vals)>leftover_pos: return np.random.choice(vals,leftover_pos)
            elif len(vals): return np.random.choice(np.tile(vals,leftover_pos//len(vals)+1),leftover_pos)
            else: return neg_random(leftover_pos)
        self.item_permute=np.array([np.random.permutation(np.append(neg_random(num_neg),pos_random(i))) for i in range(len(self.user_ids))])
        self.inv_user_ids=dict(zip(self.user_ids,range(len(self.user_ids))))
        self.inv_item_ids=dict(zip(self.item_ids,range(len(self.item_ids))))
        self.user_transform_p=user_transform_p
        
    def by_id(self, u_id,v_id):
        u,v,u_phi,v_phi=self.U_dict[u_id],self.V_dict[v_id],self.U_feats[u_id],self.V_feats[v_id]
        y=self.labels[self.inv_user_ids[u_id]][self.inv_item_ids[v_id]]
        return (u,v,u_phi,v_phi)+(u_id,v_id,)+(y,)

    def by_element_idx(self,idx): 
        item=idx%self.num_items 
        user=(idx-item)//self.num_items
        u_id,v_id=self.user_ids[user],self.item_ids[item]
        return self.by_id(u_id,v_id)
    
    def __getitem__(self,idx):        
        item_idx=idx%self.n_scores_user        
        user_idx=(idx-item_idx)//self.n_scores_user
        user_idx=self.user_permute[user_idx]
        permuted_item_id=self.item_permute[user_idx][item_idx]
        return self.by_id(self.user_ids[user_idx],permuted_item_id)

    def __len__(self): 
      return self.num_users * self.n_scores_user



class UserAnimeID(Dataset):
    def __init__(self,u_dict,v_dict,u_feats,v_feats):
        self.V_dict=v_dict
        self.U_dict=u_dict
        self.V_feats=v_feats 
        self.U_feats=u_feats
        self.V=np.array(list(v_dict.values()))
        self.U=np.array(list(u_dict.values()))
        self.V_phi=list(v_feats.values())
        self.U_phi=list(u_feats.values())
        self.labels=np.dot(self.U,self.V.T)#(num users,num items)
        self.num_users,self.num_items=self.labels.shape
        self.user_ids=list(map(int,u_dict.keys()))
        self.item_ids=list(map(int,v_dict.keys()))
    
    def __getitem__(self,idx): 
        item=idx%self.num_items 
        user=(idx-item)//self.num_items
        # u_id,v_id=self.user_item_pairs[idx]
        u_id,v_id=self.user_ids[user],self.item_ids[item]
        u,v,u_phi,v_phi=self.U_dict[u_id],self.V_dict[v_id],self.U_feats[u_id],self.V_feats[v_id]
        y=self.labels[user][item]
        return (u,v) + (u_phi,) + (v_phi,) + (u_id,v_id,)+(y,)

    def __len__(self): 
      return self.num_items * self.num_users


class UserAnimeCacheID(UserAnimeID):
    def __init__(self,u_dict,v_dict,u_feats,v_feats):
        super().__init__(u_dict,v_dict,u_feats,v_feats)
        self.perimeter_idxes=list(range(len(v_dict)))+(len(v_dict)*np.arange(1,len(u_dict))).tolist()
    def __getitem__(self, idx):
        return super().__getitem__(self.perimeter_idxes[idx])
    def __len__(self):
        return len(self.perimeter_idxes)


#@title UserAnimeIDRandomAccess

import random
class UserAnimeIDRandomAccess(Dataset):
    def __init__(self,u_dict,v_dict,u_feats,v_feats,pos_thresh=10.0,neg_thresh=-6.0,row_major=True,sample_mask=dict(),val=False):
        self.V_dict=v_dict
        self.U_dict=u_dict
        self.V_feats=v_feats 
        self.U_feats=u_feats
        self.V=np.array(list(v_dict.values()))
        self.U=np.array(list(u_dict.values()))
        self.V_phi=list(v_feats.values())
        self.U_phi=list(u_feats.values())
        self.labels=np.dot(self.U,self.V.T)#(num users,num items)
        self.num_users,self.num_items=self.labels.shape
        self.user_ids=list(map(int,u_dict.keys()))
        self.item_ids=list(map(int,v_dict.keys()))
        pmatrix=val_pref_matrix if val else pref_matrix
        pos_mask=np.logical_or(self.labels.reshape((-1,))>pos_thresh,pmatrix.values.reshape((-1,))>5)
        print("Pos:",np.sum(pos_mask))
        neg_mask=self.labels.reshape((-1,))<neg_thresh
        print("Neg:",np.sum(neg_mask))
        mask=np.logical_or(pos_mask,neg_mask)
        self.inds=np.arange(len(mask))[mask]        
        self.sample_mask=sample_mask
        mask_inds=np.zeros(len(mask))
        mask_inds[neg_mask]=2
        mask_inds[pos_mask]=1        
        mask_inds=mask_inds[mask_inds>0]
        self.pos_inds=np.arange(len(mask_inds))[mask_inds==1]
        self.neg_inds=np.arange(len(mask_inds))[mask_inds==2]
        if sample_mask:#dict with keys num_pos, num_neg
            self.pos_sample=np.random.choice(self.pos_inds,sample_mask['num_pos'],replace=False)
            self.neg_sample=np.random.choice(self.neg_inds,sample_mask['num_neg'],replace=False)
            self.mask_inds=mask_inds

        self.row_major=row_major
        self.major_len=self.num_users if row_major else self.num_items
        self.minor_len=self.num_items if row_major else self.num_users
    
    def __getitem__(self,idx): 
        if len(self.sample_mask):
            ind=self.pos_sample[idx] if idx<self.sample_mask["num_pos"] else self.neg_sample[idx-self.sample_mask["num_pos"]]
        else:
            ind=self.pos_inds[idx] if idx<len(self.pos_inds) else self.neg_inds[idx-len(self.pos_inds)]
        idx=self.inds[ind]
        major_ind=idx%self.major_len
        minor_ind=random.randrange(self.minor_len)
        inds=[minor_ind,major_ind]
        if self.row_major: inds=inds[::-1]
        user,item=inds
        u_id,v_id=self.user_ids[user],self.item_ids[item]
        u,v,u_feat,v_feat=self.U_dict[u_id],self.V_dict[v_id],self.U_feats[u_id],self.V_feats[v_id]
        y=self.labels[user][item]
        return (u,v) + (u_feat,) + (v_feat,) + (u_id,v_id,)+ (y,)
      
    def __len__(self): 
        return self.sample_mask["num_pos"]+self.sample_mask["num_neg"] if len(self.sample_mask) else len(self.pos_inds)+len(self.neg_inds)

    def labels(self):
        return self.labels

TRAIN_POS,TRAIN_NEG,VAL_POS,VAL_NEG=47664,238320,5000,25000
sample_dic={"num_pos":TRAIN_POS,"num_neg":TRAIN_NEG}
val_sample_dic={"num_pos":VAL_POS,"num_neg":VAL_NEG}

def bert_a_feats(bert_type):
    bert_path="cached/recsys/trained_bert_representations/encodings/{}_encodings.json".format(bert_type)
    return np.array(list(json.load(open(bert_path)).values()))

def bert_e_feats(bert_type):
    rec_path="cached/recsys/trained_bert_representations/rec_embeddings/{}_rec_embeddings.npy".format(bert_type)
    loaded_recs=np.load(rec_path,allow_pickle=True)
    try:
        return torch.stack(loaded_recs.tolist())[valid_rec_indices]
    except:
        pdb.set_trace()

a_feats_dic={"tf-idf":lambda x: tfidf_anime_feats,
             "bert-static":bert_a_feats,
            "bert-finetuned":bert_a_feats,
            "bert-transfer-static":bert_a_feats,
            "bert-transfer-finetuned":bert_a_feats}
e_feats_dic={"tf-idf":lambda x: tfidf_edge_feats,
             "bert-static":bert_e_feats,
            "bert-finetuned":bert_e_feats,
            "bert-transfer-static":bert_e_feats,
            "bert-transfer-finetuned":bert_e_feats}



data_inits=lambda a,e:make_data(a_feats_dic[a](a),e_feats_dic[e](e))



#@title Initialize model
import torch.optim as optim
from torch.optim.lr_scheduler import MultiplicativeLR,LambdaLR
from sklearn.model_selection import ParameterGrid as pg

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

LABELS_MEAN=0.19462400061494314
LABELS_STD=2.870017805272886
# data_mode_indices=[0,1,2,3,4,5,10,15,20,23]
config={
    "hidden_dims":[[500]],
    "latent_dim":[200],#FIX
    "anime_dim":[774],#FIX
    "num_gnn_feats":[0],
    "user_drop_p":[0.5],
    "item_corrupt_p":[0.1,0.3,0.5,0.7,0.9],
    "eup":[1.0],
    "eip":[0.0],
    "activation":["tanh"],
    "n_score_users":[36],
    "pos_neg_ratio":[5],
    "neg_thresh":[float("inf")],
    "data_mode":[('tf-idf','tf-idf')],
}
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
configs=list(pg(config))


def get_optimizer(dn):

    optimizer=optim.SGD([{'params': dn.parameters()},
                        ], lr=0.001/(LR_SCALE_FACTOR*NUM_ACCUMULATION_STEPS),momentum=MOMENTUM)
    return optimizer

#@title Learning rate adjust
def adjust_learning_rate(optimizer, epoch,rate=0.9):
    """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
    optimizer.param_groups[0]['lr'] *=LR_DECAY_RATE

#@title Plot
from matplotlib import pyplot as plt

def plot_train(losses,val=False):
    plt.title("{} Loss".format("Validation" if val else "Training"))
    plt.xlabel("Epoch")
    plt.ylabel("Relevance Loss")
    plt.plot(losses)
    plt.show()

#@title Training 

from tqdm import tqdm

def evaluate(epoch,dn,val_dload,preds=None,tru=None,eval=False):
    dn.eval()
    with torch.no_grad():
        eval_loss=0.0
        for (j,data) in tqdm(enumerate(val_dload,0)):

            u,v,u_phi,v_phi,u_ids,v_ids,y=data
            if not j: dn.update(v_ids)  
            v=np.zeros_like(v)
            a,b,c,d=torch.tensor(u,dtype=torch.float),torch.tensor(v,dtype=torch.float),torch.tensor(u_phi,dtype=torch.float),torch.tensor(v_phi,dtype=torch.float)

            z=torch.tensor(y,dtype=torch.float)
            u_ids,v_ids=u_ids.numpy(),v_ids.numpy()
            a,b,c,d,z=a.to(device),b.to(device),c.to(device),d.to(device),z.to(device)
            r,loss=dn.forward(a,b,c,d,u_ids,v_ids,z,eval=eval)
            if preds!=None and tru!=None:
                preds.extend(r.cpu().numpy())
                tru.extend(y.cpu().numpy())
            loss=loss.mean()
            eval_loss+=loss.item()
            
        print('[%d] eval loss: %.5f' %(epoch + 1,  eval_loss / (j+1)))
        return eval_loss / (j+1)

def item_recall_at_M(M, R_hat_np, val_pref_matrix):
    ranking = np.argsort(-R_hat_np, axis=1)
    topM = ranking[:,:M]

    n = np.array([len(set(a) & set(np.nonzero(b)[0])) for a, b in zip(topM, val_pref_matrix.to_numpy())])
    d = np.sum(val_pref_matrix.to_numpy() > 0, axis=1)
    
    return np.mean(n[d>0] / d[d>0])

def user_recall_at_M(M, R_hat_np, val_pref_matrix):
    ranking = np.argsort(-R_hat_np, axis=1)
    topM = ranking[:,:M]

    pref = val_pref_matrix.to_numpy()

    n = np.array([np.count_nonzero(topM[pref[:,i] > 0] == i) for i in range(pref.shape[1])])
    d = np.sum(pref > 0, axis=0)
    
    return np.mean(n[d>0] / d[d>0])

def cold_metric(n_users,n_items,config=None,dn=None):
    assert min(n_users,n_items)==0
    cache_uaid=UserAnimeCacheID(u_dict,v_dict,u_feats,v_feats)
    # sliced_uaid=SlicedUserAnimeID(u_dict,v_dict,u_feats,v_feats,slice_users=n_users,slice_items=n_items)
    dload=DataLoader(cache_uaid,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS)
    preds,tru=[],[]
    if not dn:
        assert config
        dn=DropoutGNet(**config)
        dn.load_state_dict(torch.load(PATH,map_location=device))

    dn.f_u_cache={}
    dn.f_v_cache={}
    evaluate(0,dn,dload,eval=True)
    print("CACHED")
    preds,tru=[],[]
    U_hat=torch.stack([dn.f_u_cache[k] for k in u_dict])
    V_hat=torch.stack([dn.f_v_cache[k] for k in v_dict])
    R_hat=U_hat.matmul(V_hat.T)
    R_hat_np=R_hat.cpu().numpy()
    ir=item_recall_at_M(100, R_hat_np, pref_matrix)
    ur=user_recall_at_M(100, R_hat_np, pref_matrix)
    return (ir,ur)
            
def train(dn,optimizer,num_epochs=100,cont=-1,num_accumulation_steps=NUM_ACCUMULATION_STEPS,save_every=25,metric_every=METRIC_EVERY,save_path=""):
    

    ### PICK UP WHERE IT LEFT OFF
    best_metric=float("-inf")
    preds=[]
    if os.path.isfile("{}_{}.pt".format(save_path,num_epochs-1)): 
        print("save_path","{}_{}.pt".format(save_path,num_epochs-1),"finished")
        return
    else:
        files=os.listdir(os.path.dirname(save_path))
        print(save_path)
        chkpts=[os.path.isfile("{}_{}.pt".format(save_path,save_every*i+save_every-1)) for i in range(num_epochs//save_every)]
        cont=-1
        print("training for",save_path)

    if os.path.isfile("{}_metrics.txt".format(save_path)):
        print("Exists!")
        return
    for epoch in range(cont+1,num_epochs):
        uaid=UserAnimePermuteID(u_dict,v_dict,u_feats,v_feats,n_scores_user=dn.n_score_users,pos_neg_ratio=dn.pos_neg_ratio)
        dload=DataLoader(uaid,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)

        
        dn.train(True)
        # data_iter=iter(dload) 
        epoch_loss=0.0
        if epoch >= 1:
            adjust_learning_rate(optimizer, epoch,rate=LR_DECAY_RATE)
        for (i,data) in tqdm(enumerate(dload,0)):
            if i%metric_every==0:
                if i: adjust_learning_rate(optimizer, epoch,rate=LR_DECAY_RATE)
                metrics=cold_metric(0,len(v_dict),dn=dn)
                with open("{}_metrics.txt".format(save_path),"a+") as f:
                    f.write("{},{}\n".format(*metrics))     
                    print("Save path:",save_path)                               
                    print("Metrics:",metrics)
                    best_metric=max(best_metric,metrics[0])
                    print("Best:",best_metric)

                dn.train(True)
                torch.cuda.empty_cache()
            u,v,u_phi,v_phi,u_ids,v_ids,y=data
            dn.update(v_ids) 
            
            a,b,c,d=torch.tensor(u,dtype=torch.float),torch.tensor(v,dtype=torch.float),torch.tensor(u_phi,dtype=torch.float),torch.tensor(v_phi,dtype=torch.float)

            z=torch.tensor(y,dtype=torch.float)
            u_ids,v_ids=u_ids.numpy(),v_ids.numpy()
            a,b,c,d,z=a.to(device),b.to(device),c.to(device),d.to(device),z.to(device)
            r,loss=dn.forward(a,b,c,d,u_ids,v_ids,z)
            loss=loss.mean()
            s1=list(dn.parameters())[0].clone()
            loss.backward()


            if i%num_accumulation_steps==num_accumulation_steps-1:
                optimizer.step()
                optimizer.zero_grad()
                torch.cuda.empty_cache()
            

            # assert i or (not torch.equal(s1.data,s2.data)) # only checks for i=0

            epoch_loss+=loss.item()
            torch.cuda.empty_cache()
        print('[%d] loss: %.5f' % (epoch + 1, epoch_loss / (i+1)))

        yield preds,epoch_loss/(i+1),0.0

        ###SAVE LATEST EPOCHS, BUT TFIDF TRAINS TOO FAST FOR IT TO SAVE EVERY LOSS
        ###SO JUST IGNORE FOR TFIDF, REMEMBER TO SAVE PLOTS
        # with open("{}.txt".format(save_path,epoch),"a+") as f:
        #     f.write("{},{},{}\n".format(epoch,epoch_loss/(i+1),eval_loss))

        # if epoch%save_every==save_every-1:
        #     torch.save(dn.state_dict(),"{}_{}.pt".format(save_path,epoch))
        
def run_experiment(dn,optimizer,num_epochs=100,plot_every=10,save_path=""):
    preds=None;epoch_losses=[];eval_losses=[]
    np.random.seed(0)
    random.seed(0)
  

    for _ in range(num_epochs):
        
        for (preds,t_l,v_l) in train(dn,optimizer,num_epochs,save_path=save_path):
            preds=preds
            epoch_losses.append(t_l)
            eval_losses.append(v_l)
            if len(epoch_losses)%plot_every==1:
                plot_train(epoch_losses)
            if len(epoch_losses)%plot_every==2:
                plot_train(eval_losses,True)

def checkpoint_name_from_config(path,config):
    #USAGE: path="models/tfidf_with_recs/"
    out=""
    for (k,v) in config.items():
        out+="{}={}__".format(k,v)
    out=out.rstrip("__")+".pt"
    return os.path.join(path,out)

class MyDataParallel(nn.DataParallel):
    def __getattr__(self, name):
        return getattr(self.module, name)

###RUN EXPERIMENTS OVER ALL HPARAM CONFIGS
def run_experiments(configs,path="models/recsys/tfidf/recs"):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    for config in configs:
        save_path=checkpoint_name_from_config(path,config)
        os.makedirs(path,exist_ok=True)        
        dn=DropoutGNet(**config)
        dn=dn.to(device)
        optimizer=get_optimizer(dn)        
        run_experiment(dn,optimizer,NUM_EPOCHS,plot_every=3,save_path=save_path)


run_experiments(configs,"models/recsys/anime_warm_fold")

