# -*- coding: utf-8 -*-
"""DEEPANIGNET WITH EVALUATION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MdKicPe4R2HNnKn6JlOeQDeol4vq55EW
"""

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
!pip install transformers

import pandas as pd
from collections import OrderedDict
from torch.utils.data import Dataset

import numpy as np
from abc import abstractmethod
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from typing import List
import torch
from torch.nn import functional as F

import transformers
import torch.nn as nn
import itertools
from torch.utils.data import DataLoader

# Commented out IPython magic to ensure Python compatibility.
#@title Install
# Install required packages.
!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html
!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html
!pip install -q torch-geometric

# Helper function for visualization.
# %matplotlib inline
import torch
import networkx as nx
import matplotlib.pyplot as plt


def visualize(h, color, epoch=None, loss=None):
    plt.figure(figsize=(7,7))
    plt.xticks([])
    plt.yticks([])

    if torch.is_tensor(h):
        h = h.detach().cpu().numpy()
        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap="Set2")
        if epoch is not None and loss is not None:
            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)
    else:
        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,
                         node_color=color, cmap="Set2")
    plt.show()

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

# Commented out IPython magic to ensure Python compatibility.

# %cd 'drive/MyDrive/CS224N Project'



#@title RecommendationsDataset
from collections import OrderedDict as od
class RecommendationDataset(Dataset):
    def __init__(self, index_path, animes_path, recommendations_path):
        self.neg_k=3
        
        self.rc_path=recommendations_path
        self.rc_data=pd.read_csv(self.rc_path)
        gby_show=self.rc_data.groupby(['show1'])
        gby_shows=self.rc_data.groupby(['show1','show2'])
        self.gby_show={k:np.array(v) for (k,v) in gby_show.groups.items()}
        self.gby_shows={k:np.array(v) for (k,v) in gby_shows.groups.items()}
        self.path=animes_path

        #pre-processing
        

        self.data=pd.read_csv(self.path).loc[:,"synopsis":"reviews"]
        self.num_anime=len(self.data)
        indices=list(map(int,open(index_path,'r').readlines()))
        index_range=range(len(indices))
        self.index_map=dict(zip(indices,index_range))
        self.map_index=dict(zip(index_range,indices))
        self.scalar_inds=["rating","n1","n2","n3","n4","n5"]
        self.text_inds=["synopsis","reviews"]
        scaler=MinMaxScaler()
        vals=self.data.loc[:,self.scalar_inds].values
        scaler.fit(vals)
        self.scaler=scaler
        self.numerics=self.scaler.transform(vals)
    

    def __len__(self):
        return len(self.rc_data)

    def __negsample__(self,idx):
        show1=self.rc_data.iloc[idx,0]
        idxes2=np.random.choice(self.gby[idx],self.neg_k,False)
        neg_shows=self.rc_data.iloc[idxes2,1]
        neg_show_inds=[self.index_map[s] for s in neg_shows]
        neg_mask=np.ones(self.num_anime)
        neg_mask[neg_show_inds]=0
        neg_indices=np.arange(self.num_anime)[neg_mask]
        neg_shows=list(map(lambda x:self.map_index[x],neg_indices))
        return neg_shows


    def __getitem__(self, idx):
        id_1=int(self.rc_data["show1"][idx])
        id_2=int(self.rc_data["show2"][idx])
        data_1=self.data.iloc[self.index_map[id_1]].values
        data_2=self.data.iloc[self.index_map[id_2]].values
        return np.concatenate((data_1,data_2))


from functools import reduce

class TFIDF(RecommendationDataset):
    def __init__(self,index_path,train_lookup,animes_path,recs_path,max_features=2000,mode="train",featurizer=None):
        super().__init__(index_path,train_lookup,animes_path,recs_path)
        print(super().__len__())
        #featurizer={"fzer":TfidfFeaturizer,"scaler":MinMaxScaler}
        assert mode=="train" or featurizer 
        if featurizer:
            self.fzer=featurizer["fzer"]
            self.scaler=featurizer["scaler"]
        else:
            self.tf_config={
                'strip_accents':'ascii',
                'stop_words':'english',
                'max_features':max_features
            }
            self.dic=self.data.loc[:,self.text_inds]
            self.fzer=self.fit_text(TFIDF.get_text(self.dic))
            self.scalar_dic=self.data.loc[:,self.scalar_inds]
            self.scaler=self.fit_scalars(self.scalar_dic)

    def top_words(self):
        return self.fzer.vocabulary_

    @staticmethod
    def get_text(dic,text_inds=["synopsis","reviews","recs"]):
        args=[dic[key].values for key in text_inds]
        text=[reduce(lambda cum,cur:str(cum)+str(cur),x) for x in zip(*args)]
        return text

    def __len__(self):
        return len(self.data)

    def fit_text(self,text:List[str]):
        fzer=TfidfVectorizer(**self.tf_config)
        fzer.fit(text)
        return fzer
    
    def fit_scalars(self,scalars:List[float]):
        scaler=MinMaxScaler()
        scaler.fit(scalars)
        return scaler


    def featurize_anime(self,item):
        feats=self.featurize([str(item.synopsis)+str(item.reviews)])
        scalars=self.scale([item.loc[self.scalar_inds]])
        x=np.concatenate((scalars[0],feats[0].toarray()[0]))
        return x.astype(float)
    
    def featurize(self, text: List[str]):
        #assume fzer already transformed on train
        return self.fzer.transform(text)
    
    def scale(self, scalars):
        #same for scaler
        return self.scaler.transform(scalars)

    def getanime(self, idx):
        item=self.data.iloc[idx]
        return self.featurize_anime(item)

    def __getitem__(self, idx):
        id_1=int(self.rc_data["show1"][idx])
        id_2=int(self.rc_data["show2"][idx])
        idx1=self.index_map[id_1]
        idx2=self.index_map[id_2]
        x=np.concatenate((self.getanime(idx1),self.getanime(idx2)))
        return torch.tensor(x.astype(float),dtype=torch.float)

#@title Pretrained models
TRANSFORMER_PRETRAINED_MODELS = {
    'bert-base-uncased': {'tokenizer': transformers.BertTokenizerFast, 'model': transformers.BertModel},
    'distilbert-base-uncased': {'tokenizer': transformers.DistilBertTokenizerFast, 'model': transformers.DistilBertModel},
    'roberta-base': {'tokenizer': transformers.RobertaTokenizer, 'model': transformers.RobertaModel},
    'google/electra-small-discriminator': {'tokenizer': transformers.ElectraTokenizer, 'model': transformers.ElectraModel},
    'google/electra-small-discriminator': {'tokenizer': transformers.ElectraTokenizer, 'model': transformers.ElectraModel},
    'pretraining/distillbert-anime2vec': {'tokenizer': transformers.DistilBertTokenizerFast, 'model': transformers.DistilBertModel},
    'pretraining/distillbert-anime2vec-2': {'tokenizer': transformers.DistilBertTokenizerFast, 'model': transformers.DistilBertModel},
    'pretraining/distillbert-anime2vec-3': {'tokenizer': transformers.DistilBertTokenizerFast, 'model': transformers.DistilBertModel},
    'pretraining/distillbert-anime2vec-4': {'tokenizer': transformers.DistilBertTokenizerFast, 'model': transformers.DistilBertModel},
}

#@title TransformerFinetuneDataset

class TransformerFinetuneDataset(RecommendationDataset):
    def __init__(self, index_path, animes_path, recs_path, transformer_config='bert-base-uncased', mode="train",featurizer=None):
        super().__init__(index_path,animes_path,recs_path)
        assert mode=="train" or featurizer 
        

        if featurizer:
            self.tokenizer=featurizer["tokenizer"]
            self.scaler=featurizer["scaler"]
        else:
            self.dic=self.data.loc[:,self.text_inds]
            self.transformer_config = transformer_config
            # self.device = device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.tokenizer = TRANSFORMER_PRETRAINED_MODELS[transformer_config]['tokenizer'].from_pretrained(transformer_config)
            self.scalar_dic=self.data.loc[:,self.scalar_inds]
            self.scaler=self.fit_scalars(self.scalar_dic)

    @staticmethod
    def get_text(dic):
        synopses=dic.synopsis.values
        reviews=dic.reviews.values
        
        text=[str(s)+str(r) for (s,r) in zip(synopses,reviews)]
        return text

    def __len__(self):
        return len(self.data)
    
    def fit_scalars(self,scalars:List[float]):
        scaler=MinMaxScaler()
        scaler.fit(scalars)
        return scaler

    def featurize_anime(self,item):
        item_feats = str(item.synopsis)+str(item.reviews)
        item_encodings = self.tokenizer(item_feats, return_tensors='pt', padding='max_length', truncation=True, max_length=512)
        input_ids = torch.squeeze(item_encodings['input_ids'])
        attention_mask = torch.squeeze(item_encodings['attention_mask'])
        scalars=self.scale([item.loc[self.scalar_inds]])
        scalars=torch.squeeze(torch.tensor(scalars,dtype=torch.float))
        return input_ids, attention_mask, scalars
    
    def scale(self, scalars):
        #same for scaler
        return self.scaler.transform(scalars)

    def getanime(self, idx):
        item=self.data.iloc[idx]
        return self.featurize_anime(item)

    def __getitem__(self, idx):
        id_1=int(self.rc_data["show1"][idx])
        id_2=int(self.rc_data["show2"][idx])
        idx1=self.index_map[id_1]
        idx2=self.index_map[id_2]
        return self.getanime(idx1) + self.getanime(idx2)


train_path="train_val_test/10000/10000_animes_train.csv"
val_path="train_val_test/10000/10000_animes_val.csv"
test_path="train_val_test/10000/10000_animes_test.csv"

recs_path="train_val_test/10000/10000_recs.csv"
lookup_path="train_val_test/10000/10000_shows_to_process.txt"
train_lookup="train_val_test/10000/10000_train.txt"
rec=TransformerFinetuneDataset(lookup_path,train_path,recs_path, transformer_config='pretraining/distillbert-anime2vec-4')

import pdb

#@title DropoutNet

class DropoutNet(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()
        assert {"item_drop_p","user_drop_p","latent_dim","hidden_dim"}.issubset(set(kwargs.keys()))

        for (k,v) in kwargs.items():
            setattr(self,k,v)
        self.dim=self.latent_dim
        self.V_dict=v_dict
        self.U_dict=u_dict
        # self.item_drop_p=1.0
        # self.user_drop_p=1.0
        self.item_drop=nn.Dropout(self.item_drop_p)
        self.user_drop=nn.Dropout(self.user_drop_p)
        #LAYERS
        self.linear_u=nn.Linear(self.dim,self.hidden_dim)
        self.linear_v=nn.Linear(self.dim,self.hidden_dim)
        self.linear_phi_u=nn.Linear(self.anime_dim,self.hidden_dim)
        self.linear_phi_v=nn.Linear(self.anime_dim,self.hidden_dim)

        activation_options={"relu":nn.ReLU(),"sigmoid":nn.Sigmoid(),"tanh":nn.Tanh(),"identity":nn.Identity()}
        

        self.non_linearity=activation_options[self.activation] if "activation" in kwargs else nn.Identity()
        self.f_u=nn.Linear(2*self.hidden_dim,self.dim)
        self.f_v=nn.Linear(2*self.hidden_dim,self.dim)


    def forward(self,u,v,u_phi,v_phi,y):
        #note we're using ids, not indices
        B=u.shape[0]
        v=self.item_drop(v) #(B,h) 
        u=self.user_drop(u)

        

        u=self.linear_u(u) #(B,h)
        v=self.linear_v(v) #(B,h)
        phi_u=self.linear_phi_u(u_phi) #(B,h) 
        phi_v=self.linear_phi_v(v_phi) #(B,h)
        u=self.non_linearity(u)
        v=self.non_linearity(v)
        phi_u=self.non_linearity(phi_u)
        phi_v=self.non_linearity(phi_v)
        f_u=torch.cat((u,phi_u),axis=1)
        f_v=torch.cat((v,phi_v),axis=1)
        f_u=self.f_u(f_u)#(B,dim)
        f_v=self.f_v(f_v)#(B,dim)
        f_u=self.non_linearity(f_u)
        f_v=self.non_linearity(f_v)
        f_u=f_u.view(B,1,self.dim)
        f_v=f_v.view(B,self.dim,1)
        out=torch.bmm(f_u,f_v)
        # out=self.sigmoid(out)#(B,1,1)
        out=out.view(B,)
        loss=F.mse_loss(out,y)
        return out, loss

#@title DropoutGNet


class DropoutGNet(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()
        for (k,v) in kwargs.items():
            setattr(self,k,v)

        self.dim=self.latent_dim
        self.V_dict=v_dict
        self.U_dict=u_dict
        
        self.data=data_inits(*self.data_mode)
        self.node_dim=self.data.x.size(-1)
        self.item_drop=nn.Dropout(self.drop_p)
        self.user_drop=nn.Dropout(self.drop_p)
        #LAYERS
        self.linear_u=nn.Linear(self.dim,self.hidden_dim)
        self.linear_v=nn.Linear(self.dim,self.hidden_dim)
        self.linear_phi_u=nn.Linear(self.anime_dim,self.hidden_dim)
        self.linear_phi_v=nn.Linear(self.anime_dim+self.num_gnn_feats,self.hidden_dim)
        self.relu=nn.ReLU()
        self.f_u=nn.Linear(2*self.hidden_dim,self.dim)
        self.f_v=nn.Linear(2*self.hidden_dim,self.dim)

        if self.edge_attr:
            self.edge_feats=self.data.edge_attr
            self.edge_embed=nn.Linear(self.edge_feats.size(-1),self.node_dim)
            self.gine=GINEConv(nn.Sequential(
              OrderedDict([
                ('linear1', nn.Linear(self.data.num_features,3*self.num_gnn_feats)),
                ('tanh1', nn.Tanh()),
                ('linear2', nn.Linear(3*self.num_gnn_feats,2*self.num_gnn_feats)),
                ('tanh2', nn.Tanh()),
                ('linear3', nn.Linear(2*self.num_gnn_feats,self.num_gnn_feats)),
                ('tanh3', nn.Tanh())
              ])
            ))
        else:
            
            self.conv1=GCNConv(self.data.num_features, 3*self.num_gnn_feats)
            self.conv2=GCNConv(3*self.num_gnn_feats,2*self.num_gnn_feats)
            self.conv3=GCNConv(2*self.num_gnn_feats,self.num_gnn_feats)

        self.f_u_cache={}
        self.f_v_cache={}

    def mask_edge_inds(self,x,v_inds):
        bool_mask1=torch.tile(self.data.edge_index[0,:],(v_inds.shape[0],1))==v_inds.reshape(-1,1)
        bool_mask2=torch.tile(self.data.edge_index[1,:],(v_inds.shape[0],1))==v_inds.reshape(-1,1)
        bool_mask1=torch.sum(bool_mask1,axis=0)>0
        bool_mask2=torch.sum(bool_mask2,axis=0)>0
        bool_mask=torch.logical_and(bool_mask1,bool_mask2)
        masked_inds=torch.squeeze(torch.nonzero(bool_mask),dim=-1)
        return masked_inds
    
    def update(self,v_inds):
        x=self.data.x.to(self.device)
        
        masked_inds=self.mask_edge_inds(x,v_inds)
        edge_inds=self.data.edge_index[:,masked_inds].to(device)#(2,num rel edges)
        
        if self.edge_attr:
            #gineconv
            edge_feats=self.edge_feats[masked_inds,:].float().to(device)
            edge_attrs=self.edge_embed(edge_feats)          

            x=self.gine(x,edge_index=edge_inds,edge_attr=edge_attrs)
        else:
            x=self.conv1(x,edge_inds)
            x=x.tanh()
            x=self.conv2(x,edge_inds)
            x=x.tanh()
            x=self.conv3(x,edge_inds)
            x=x.tanh()

        if device.type=="cuda": x=x.cpu()
        x=x.detach().numpy()
        x=torch.as_tensor(stats.zscore(x))
        self.node_reprs=x.to(device)

        

    def forward(self,u,v,u_phi,v_phi,u_inds,v_inds,y,eval=False):
        B=u.shape[0]
        if eval:
            u_ind_mask=np.array(list(filter(lambda i:u_inds[i] not in self.f_u_cache,range(len(u_inds)))))

            v_ind_mask=np.array(list(filter(lambda i:v_inds[i] not in self.f_v_cache,range(len(v_inds)))))

            
            u,u_phi,v,v_phi=u[u_ind_mask],u_phi[u_ind_mask],v[v_ind_mask],v_phi[v_ind_mask]
            u_ind_mask,v_ind_mask=u_ind_mask.astype(int),v_ind_mask.astype(int)
            nodes=self.node_reprs
            nodes=nodes[v_inds[v_ind_mask]]
            v_phi=torch.cat((v_phi,nodes),axis=1)
            
            v=self.item_drop(v)
            u=self.user_drop(u)
            u=self.linear_u(u) 
            v=self.linear_v(v) 
            phi_u=self.linear_phi_u(u_phi)
            phi_v=self.linear_phi_v(v_phi)
            
            
            f_u=torch.cat((u,phi_u),axis=1)
            f_v=torch.cat((v,phi_v),axis=1)
            f_u=self.f_u(f_u)
            f_v=self.f_v(f_v)


            
            for (u_ind,f_u_) in dict(zip(u_inds[u_ind_mask],f_u)).items():
                self.f_u_cache[u_ind]=f_u_
            for (v_ind,f_v_) in dict(zip(v_inds[v_ind_mask],f_v)).items():
                self.f_v_cache[v_ind]=f_v_
            

            f_u=torch.stack([self.f_u_cache[u_ind] for u_ind in u_inds])
            
            f_v=torch.stack([self.f_v_cache[v_ind] for v_ind in v_inds])

            
        else:
            nodes=self.node_reprs
            nodes=nodes[v_inds]
            v_phi=torch.cat((v_phi,nodes),axis=1)
            
            
            v=self.item_drop(v) #(B,h) 
            u=self.user_drop(u)
            u=self.linear_u(u) #(B,h)
            v=self.linear_v(v) #(B,h)
            phi_u=self.linear_phi_u(u_phi) #(B,h) 
            phi_v=self.linear_phi_v(v_phi) #(B,h)
            
            
            f_u=torch.cat((u,phi_u),axis=1)
            f_v=torch.cat((v,phi_v),axis=1)
            f_u=self.f_u(f_u)#(B_u,dim)
            f_v=self.f_v(f_v)#(B_v,dim)

        f_u=f_u.view(B,1,self.dim)
        f_v=f_v.view(B,self.dim,1)
        out=torch.bmm(f_u,f_v)
        # out=self.sigmoid(out)#(B,1,1)
        out=out.view(B,)
        loss=F.mse_loss(out,y)

        return out, loss

#@title TransformerNet

class TransformerNet(nn.Module):

    def __init__(self, transformer_config='bert-base-uncased', finetune=False):
        super(TransformerNet, self).__init__()
        self.transformer_config = transformer_config
        self.finetune = finetune
        # self.tokenizer = TRANSFORMER_PRETRAINED_MODELS[transformer_config]['tokenizer'].from_pretrained(transformer_config)
        self.model = TRANSFORMER_PRETRAINED_MODELS[transformer_config]['model'].from_pretrained(transformer_config, return_dict=True)
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        # self.model = self.model.to(device)
        if finetune:
            self.model.train()
        else:
            self.model.eval()

    def forward(self, v_input_ids, v_attention_mask, v_scalars, u_input_ids, u_attention_mask, u_scalars, num_shows):
        batch_size, max_shows, tokenize_dim = u_input_ids.size()

        v_encodings = self.encode(v_input_ids, v_attention_mask)
        v_phi = torch.cat((v_encodings, v_scalars), dim=1)
        embedding_dim = v_phi.size(1)

        u_input_ids = u_input_ids.view(batch_size * max_shows, tokenize_dim)
        u_attention_mask = u_attention_mask.view(batch_size * max_shows, tokenize_dim)
        u_encodings = self.encode(u_input_ids, u_attention_mask)
        u_encodings = u_encodings.view(batch_size, max_shows, 768)
        # print(u_encodings.dtype, u_scalars.dtype)
        u_phi = torch.cat((u_encodings, u_scalars), dim=2)
        

        mask = torch.arange(max_shows)[None, :].to(self.device) < num_shows[:, None]
        mask = mask.type(torch.float)
        try:
            mask = torch.repeat_interleave(mask[:, :, None], embedding_dim, dim=-1)
            
            u_phi = u_phi * mask
            u_phi = u_phi.sum(dim=1)
            u_phi = u_phi / num_shows[:, None]
        except:
            pdb.set_trace()

        return u_phi, v_phi
    
    def encode(self, input_ids, attention_mask):
        encoding = self.model(input_ids=input_ids, attention_mask=attention_mask)
        # return encoding.last_hidden_state[:, 0, :]
        encoding = encoding.last_hidden_state[:, 0, :]
        # breakpoint()
        means = encoding.mean(dim=1, keepdim=True)
        stds = encoding.std(dim=1, keepdim=True)
        normalized_encoding = (encoding - means) / stds
        return normalized_encoding

#@title DropoutTransformerNet
class DropoutTransformerNet(nn.Module):
    def __init__(self, transformer_config='bert-base-uncased', finetune=False, **kwargs):
        super(DropoutTransformerNet, self).__init__()
        self.encoder = TransformerNet(transformer_config, finetune)
        self.dropoutnet = DropoutNet(**kwargs)
    
    def forward(self, u, v, u_input_ids, u_attention_mask, u_scalars, num_shows, v_input_ids, v_attention_mask, v_scalars,  y):
        u_phi, v_phi = self.encoder(v_input_ids, v_attention_mask, v_scalars, u_input_ids, u_attention_mask, u_scalars, num_shows)
        out, loss = self.dropoutnet(u, v, u_phi, v_phi, y)
        return out, loss

#@title DropoutTransformerGNet
class DropoutTransformerNet(nn.Module):
    def __init__(self, transformer_config='bert-base-uncased', finetune=False, **kwargs):
        super(DropoutTransformerNet, self).__init__()
        self.encoder = TransformerNet(transformer_config, finetune)
        self.dropoutgnet = DropoutGNet(**kwargs)
        self.u_phi_cache={}
        self.v_phi_cache={}

    def update(self,v_inds):
        self.dropoutgnet.update(v_inds)
    
    def forward(self, u, v, u_input_ids, u_attention_mask, u_scalars, num_shows, v_input_ids, v_attention_mask, v_scalars, u_ids, v_ids,y,eval=False):
        if eval:
            u_id_mask=np.array(list(filter(lambda i:u_ids[i] not in self.u_phi_cache,range(len(u_ids)))))

            u_input_ids,u_attention_mask,u_scalars=u_input_ids[u_id_mask],u_attention_mask[u_id_mask],u_scalars[u_id_mask]
            v_id_mask=np.array(list(filter(lambda i:v_ids[i] not in self.v_phi_cache,range(len(v_ids)))))

            
            v_input_ids,v_attention_mask,v_scalars=v_input_ids[v_id_mask],v_attention_mask[v_id_mask],v_scalars[v_id_mask]
            num_shows=num_shows[u_id_mask]
            u_phi, v_phi = self.encoder(v_input_ids, v_attention_mask, v_scalars, u_input_ids, u_attention_mask, u_scalars, num_shows)
            #unstack to iterate
            u_id_mask,v_id_mask=u_id_mask.astype(int),v_id_mask.astype(int)
            for (u_id,u_phi_) in dict(zip(u_ids[u_id_mask],u_phi)).items():
                self.u_phi_cache[u_id]=u_phi_
            for (v_id,v_phi_) in dict(zip(v_ids[v_id_mask],v_phi)).items():
                self.v_phi_cache[v_id]=v_phi_ 
            

            u_phi=torch.stack([self.u_phi_cache[u_id] for u_id in u_ids])
            
            v_phi=torch.stack([self.v_phi_cache[v_id] for v_id in v_ids])

            

        else:    
            u_phi, v_phi = self.encoder(v_input_ids, v_attention_mask, v_scalars, u_input_ids, u_attention_mask, u_scalars, num_shows)
        
            
        
        out, loss = self.dropoutgnet(u, v, u_phi, v_phi, u_ids,v_ids,y,eval)
        return out, loss

#@title Load feats

user_path="train_val_test/10000/10000_train_user_factors.csv"
item_path="train_val_test/10000/10000_train_item_factors.csv"
pref_path="train_val_test/10000/10000_train_pref_matrix.csv"
val_pref_path="train_val_test/10000/10000_val_pref_matrix.csv"
test_pref_path="train_val_test/10000/10000_test_pref_matrix.csv"

train_ids="train_val_test/10000/10000_train.txt"
val_ids="train_val_test/10000/10000_val.txt"
test_ids="train_val_test/10000/10000_test.txt"

user_df=pd.read_csv(user_path)
item_df=pd.read_csv(item_path)

val_item_df=pd.read_csv(val_path)

train_ids=list(map(lambda x:rec.index_map[int(x)],open(train_ids).readlines()))#inds, not ids
val_ids=list(map(lambda x:rec.index_map[int(x)],open(val_ids).readlines()))#inds, not ids
test_ids=list(map(lambda x:rec.index_map[int(x)],open(test_ids).readlines()))#inds, not ids

user_ids=user_df.iloc[:,0]#inds
item_vectors=pd.read_csv(item_path).iloc[:,1:]
user_vectors=pd.read_csv(user_path).iloc[:,1:]

pref_matrix=pd.read_csv(pref_path).iloc[:,1:]
val_pref_matrix=pd.read_csv(val_pref_path).iloc[:,1:]
pref_mask=pref_matrix>0.0
val_pref_mask=val_pref_matrix>0.0

u_dict=dict(zip(user_ids,user_vectors.values))#{user_ind:item_vec}
v_dict=dict(zip(train_ids,item_vectors.values))#{anime_ind:item_vec}

#@title get_u_feat
# pref_matrix=pd.read_csv(pref_path).iloc[:,1:]
# # val_pref_matrix=pd.read_csv(val_pref_path).iloc[:,1:]
# pref_mask=pref_matrix>0.0
# # val_pref_mask=val_pref_matrix>0.0

max_shows_per_user = int(max(np.array(pref_mask, dtype=np.float).sum(axis=1)))
max_shows_per_user = 5

all_v_feats=[rec.getanime(i) for i in range(len(item_df))]
v_feats=dict(zip(train_ids,all_v_feats))
all_u_transforms=[np.average(item_vectors[pref_mask.iloc[i].values],axis=0) for i in range(len(user_df))]
u_transforms=dict(zip(user_ids,all_u_transforms))

tokenizer = transformers.BertTokenizer.from_pretrained('pretraining/distillbert-anime2vec-4')
empty_encodings = tokenizer("", return_tensors='pt', padding='max_length', truncation=True, max_length=512)
empty_input_ids = torch.squeeze(empty_encodings['input_ids'])
empty_attention_mask = torch.squeeze(empty_encodings['attention_mask'])
empty_scalars = torch.tensor([0, 0, 0, 0, 0, 0], dtype=float)

def get_u_feat(i):
    u_input_ids = []
    u_attention_mask = []
    u_scalars = []
    for j, included in enumerate(pref_mask.iloc[i].values):
        if included:
            input_ids, attention_mask, scalars = all_v_feats[j]
            u_input_ids.append(input_ids)
            u_attention_mask.append(attention_mask)
            u_scalars.append(scalars)
        if len(u_input_ids) >= max_shows_per_user:
            break

    num_shows_per_user = len(u_input_ids)
    if num_shows_per_user < max_shows_per_user:
        u_input_ids = u_input_ids + (max_shows_per_user - num_shows_per_user) * [empty_input_ids]
        u_attention_mask = u_attention_mask + (max_shows_per_user - num_shows_per_user) * [empty_attention_mask]
        u_scalars = u_scalars + (max_shows_per_user - num_shows_per_user) * [empty_scalars]
    # else:
    #     u_input_ids = u_input_ids[:max_shows_per_user]
    #     u_attention_mask = u_attention_mask[:max_shows_per_user]
    #     u_scalars = u_scalars[:max_shows_per_user]
    u_input_ids = torch.stack(u_input_ids)
    u_attention_mask = torch.stack(u_attention_mask)
    u_scalars = torch.stack(u_scalars)
    u_scalars = u_scalars.type(torch.float)
    return u_input_ids, u_attention_mask, u_scalars, num_shows_per_user
    

all_u_feats=[get_u_feat(i) for i in range(len(user_df))]
u_feats=dict(zip(user_ids,all_u_feats))
all_val_v_feats=[rec.featurize_anime(val_item_df.iloc[i]) for i in range(len(val_item_df))]
val_v_feats=dict(zip(val_ids,all_val_v_feats))

u_indexers=[np.arange(len(user_df))[mask] for mask in val_pref_mask.T.values]
indexer_avg=np.array([np.average(user_vectors.iloc[u_index,:],axis=0) for u_index in u_indexers])
val_v_dict=dict(zip(val_ids,indexer_avg))

#@title Load cache
import json
path="cached/tfidf/10000/"
VAR_NAMES=['u_dict','v_dict','val_v_dict']
for j in VAR_NAMES:
    with open(path+j+".json") as f:
        dic=json.load(f)
        locals()[j]={int(float(k)):np.array(dic[k]) for k in dic}

assert len(v_dict)==8000

#@title UserAnimeIDRandomAccess

import random
class UserAnimeIDRandomAccess(Dataset):
    def __init__(self,u_dict,v_dict,u_feats,v_feats,pos_thresh=10.0,neg_thersh=-6.0,row_major=True,sample_mask=dict()):
        self.V_dict=v_dict
        self.U_dict=u_dict
        self.V_feats=v_feats 
        self.U_feats=u_feats
        self.V=np.array(list(v_dict.values()))
        self.U=np.array(list(u_dict.values()))
        self.V_phi=list(v_feats.values())
        self.U_phi=list(u_feats.values())
        self.labels=np.dot(self.U,self.V.T)#(num users,num items)
        self.num_users,self.num_items=self.labels.shape
        self.user_ids=list(map(int,u_dict.keys()))
        self.item_ids=list(map(int,v_dict.keys()))

        pos_mask=self.labels.reshape((-1,))>pos_thresh
        print("Pos:",np.sum(pos_mask))
        neg_mask=self.labels.reshape((-1,))<neg_thersh
        print("Neg:",np.sum(neg_mask))
        mask=np.logical_or(pos_mask,neg_mask)
        self.inds=np.arange(len(mask))[mask]
        
        self.sample_mask=sample_mask
        if sample_mask:#dict with keys num_pos, num_neg
            mask_inds=np.zeros(len(mask))
            mask_inds[pos_mask]=1
            mask_inds[neg_mask]=2
            mask_inds=mask_inds[mask_inds>0]
            self.pos_inds=np.arange(len(mask_inds))[mask_inds==1]
            self.neg_inds=np.arange(len(mask_inds))[mask_inds==2]
            self.pos_sample=np.random.choice(self.pos_inds,sample_mask['num_pos'],replace=False)
            self.neg_sample=np.random.choice(self.neg_inds,sample_mask['num_neg'],replace=False)
            self.mask_inds=mask_inds

        self.row_major=row_major
        self.major_len=self.num_users if row_major else self.num_items
        self.minor_len=self.num_items if row_major else self.num_users
        # mask=np.random.choice([True,False],size=len(user_ids)*len(item_ids),p=[0.1,0.9])
        # breakpoint()
        # mask=np.arange(len(user_ids)*len(item_ids))[mask]
        # self.user_item_pairs=np.array(list(itertools.product(user_ids,item_ids)))[mask]
    
    def __getitem__(self,idx): 
        if idx<self.sample_mask["num_pos"]:
            ind=self.pos_sample[idx]
        else:
            ind=self.neg_sample[idx-self.sample_mask["num_pos"]]
        idx=self.inds[ind]
        major_ind=idx%self.major_len
        minor_ind=random.randrange(self.minor_len)
        inds=[minor_ind,major_ind]
        if self.row_major: inds=inds[::-1]
        user,item=inds
        u_id,v_id=self.user_ids[user],self.item_ids[item]
        u,v,u_feat,v_feat=self.U_dict[u_id],self.V_dict[v_id],self.U_feats[u_id],self.V_feats[v_id]
        y=self.labels[user][item]
        # return (u,v,u_phi,v_phi), y
        return (u,v) + u_feat + v_feat + (u_id,v_id,)+ (y,)
      
    def __len__(self): 
        return self.sample_mask["num_pos"]+self.sample_mask["num_neg"]

    def labels(self):
        return self.labels

TRAIN_POS,TRAIN_NEG,VAL_POS,VAL_NEG=247,186,74,50
sample_dic={"num_pos":TRAIN_POS,"num_neg":TRAIN_NEG}
val_sample_dic={"num_pos":VAL_POS,"num_neg":VAL_NEG}
uaid=UserAnimeIDRandomAccess(u_dict,v_dict,u_feats,v_feats,sample_mask=sample_dic)
val_uaid=UserAnimeIDRandomAccess(u_dict,val_v_dict,u_feats,val_v_feats,9.999999,sample_mask=val_sample_dic)

#@title Load graph data
from sklearn.preprocessing import OneHotEncoder
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, GINEConv
import scipy
from scipy import stats

enc=OneHotEncoder()
one_hot_anime_feats=np.arange(10000)
one_hot_anime_feats=enc.fit_transform(one_hot_anime_feats.reshape(-1,1)).toarray()
tfidf_anime_feats=np.load("cached/recsys/tfidf_anime_feats.npy",allow_pickle=False)
edge_inds=np.load("cached/recsys/edge_inds.npy",allow_pickle=False)
tfidf_edge_feats=np.load("cached/recsys/tfidf_edge_feats.npy",allow_pickle=False)
make_data=lambda a_feats,e_feats:Data(torch.as_tensor(a_feats,dtype=torch.float32),
                                   torch.as_tensor(edge_inds.T,dtype=torch.long),
                                   torch.as_tensor(e_feats))



def bert_a_feats(bert_type):
    bert_path="cached/recsys/trained_bert_representations/encodings/{}_encodings.json".format(bert_type)
    return np.array(list(json.load(open(bert_path)).values()))

def bert_e_feats(bert_type):
    rec_path="cached/recsys/trained_bert_representations/rec_embeddings/{}_rec_embeddings.npy".format(bert_type)
    loaded_recs=np.load(rec_path,allow_pickle=True)
    try:
        return torch.stack(loaded_recs.tolist())
    except:
        pdb.set_trace()

a_feats_dic={"tf-idf":lambda x: tfidf_anime_feats,
             "bert-static":bert_a_feats,
            "bert-finetuned":bert_a_feats,
            "bert-transfer-static":bert_a_feats,
            "bert-transfer-finetuned":bert_a_feats}
e_feats_dic={"tf-idf":lambda x: tfidf_edge_feats,
             "bert-static":bert_e_feats,
            "bert-finetuned":bert_e_feats,
            "bert-transfer-static":bert_e_feats,
            "bert-transfer-finetuned":bert_e_feats}



data_inits=lambda a,e:make_data(a_feats_dic[a](a),e_feats_dic[e](e))

#@title Initialize model
import torch.optim as optim
from torch.optim.lr_scheduler import MultiplicativeLR,LambdaLR
from sklearn.model_selection import ParameterGrid as pg

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


data_mode_indices=[0,1,2,3,4,5,10,15,20,23]
config={
    "bert":["pretrained_finetune"],#+["static","pretrained_static","finetune",]
    "device":[device],
    "hidden_dim":[100],
    "latent_dim":[10],#FIX
    "anime_dim":[774],#FIX
    "num_gnn_feats":[32],
    "drop_p":[0.5,0.75],
    
    "edge_attr":[True],
    "data_mode":np.array(list(itertools.product(a_feats_dic.keys(),e_feats_dic.keys())))[data_mode_indices]
}
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
configs=list(pg(config))[16:20]

#@title BERT params 
gnn_param_names={
    
}
fine_tune_param_names={ 'encoder.model.transformer.layer.5.attention.q_lin.weight',
 'encoder.model.transformer.layer.5.attention.q_lin.bias',
 'encoder.model.transformer.layer.5.attention.k_lin.weight',
 'encoder.model.transformer.layer.5.attention.k_lin.bias',
 'encoder.model.transformer.layer.5.attention.v_lin.weight',
 'encoder.model.transformer.layer.5.attention.v_lin.bias',
 'encoder.model.transformer.layer.5.attention.out_lin.weight',
 'encoder.model.transformer.layer.5.attention.out_lin.bias',
 'encoder.model.transformer.layer.5.sa_layer_norm.weight',
 'encoder.model.transformer.layer.5.sa_layer_norm.bias',
 'encoder.model.transformer.layer.5.ffn.lin1.weight',
 'encoder.model.transformer.layer.5.ffn.lin1.bias',
 'encoder.model.transformer.layer.5.ffn.lin2.weight',
 'encoder.model.transformer.layer.5.ffn.lin2.bias',
 'encoder.model.transformer.layer.5.output_layer_norm.weight',
 'encoder.model.transformer.layer.5.output_layer_norm.bias',
 'dropoutgnet.linear_u.weight',
 'dropoutgnet.linear_u.bias',
 'dropoutgnet.linear_v.weight',
 'dropoutgnet.linear_v.bias',
 'dropoutgnet.linear_phi_u.weight',
 'dropoutgnet.linear_phi_u.bias',
 'dropoutgnet.linear_phi_v.weight',
 'dropoutgnet.linear_phi_v.bias',
 'dropoutgnet.f_u.weight',
 'dropoutgnet.f_u.bias',
 'dropoutgnet.f_v.weight',
 'dropoutgnet.f_v.bias',
 'dropoutgnet.edge_embed.weight',
 'dropoutgnet.edge_embed.bias',
 'dropoutgnet.gine.nn.linear1.weight',
 'dropoutgnet.gine.nn.linear1.bias',
 'dropoutgnet.gine.nn.linear2.weight',
 'dropoutgnet.gine.nn.linear2.bias',
 'dropoutgnet.gine.nn.linear3.weight',
 'dropoutgnet.gine.nn.linear3.bias'
}
fine_tune_bert_param_names={ 'encoder.model.transformer.layer.5.attention.q_lin.weight',
 'encoder.model.transformer.layer.5.attention.q_lin.bias',
 'encoder.model.transformer.layer.5.attention.k_lin.weight',
 'encoder.model.transformer.layer.5.attention.k_lin.bias',
 'encoder.model.transformer.layer.5.attention.v_lin.weight',
 'encoder.model.transformer.layer.5.attention.v_lin.bias',
 'encoder.model.transformer.layer.5.attention.out_lin.weight',
 'encoder.model.transformer.layer.5.attention.out_lin.bias',
 'encoder.model.transformer.layer.5.sa_layer_norm.weight',
 'encoder.model.transformer.layer.5.sa_layer_norm.bias',
 'encoder.model.transformer.layer.5.ffn.lin1.weight',
 'encoder.model.transformer.layer.5.ffn.lin1.bias',
 'encoder.model.transformer.layer.5.ffn.lin2.weight',
 'encoder.model.transformer.layer.5.ffn.lin2.bias',
 'encoder.model.transformer.layer.5.output_layer_norm.weight',
 'encoder.model.transformer.layer.5.output_layer_norm.bias'
}
fine_tune_dropoutnet_param_names={
 'dropoutgnet.linear_u.weight',
 'dropoutgnet.linear_u.bias',
 'dropoutgnet.linear_v.weight',
 'dropoutgnet.linear_v.bias',
 'dropoutgnet.linear_phi_u.weight',
 'dropoutgnet.linear_phi_u.bias',
 'dropoutgnet.linear_phi_v.weight',
 'dropoutgnet.linear_phi_v.bias',
 'dropoutgnet.f_u.weight',
 'dropoutgnet.f_u.bias',
 'dropoutgnet.f_v.weight',
 'dropoutgnet.f_v.bias',
 'dropoutgnet.edge_embed.weight',
 'dropoutgnet.edge_embed.bias',
 'dropoutgnet.gine.nn.linear1.weight',
 'dropoutgnet.gine.nn.linear1.bias',
 'dropoutgnet.gine.nn.linear2.weight',
 'dropoutgnet.gine.nn.linear2.bias',
 'dropoutgnet.gine.nn.linear3.weight',
 'dropoutgnet.gine.nn.linear3.bias'
}
def get_optimizer(dn):
    fine_tune_params=list(map(lambda x:x[1],filter(lambda x:x[0] in fine_tune_param_names,dn.named_parameters())))
    regular_params=list(map(lambda x:x[1],filter(lambda x:x[0] not in fine_tune_param_names,dn.named_parameters())))
    for regular_param in regular_params:
        regular_param.requires_grad = False
    fine_tune_bert_params=list(map(lambda x:x[1],filter(lambda x:x[0] in fine_tune_bert_param_names,dn.named_parameters())))
    fine_tune_dropoutnet_params=list(map(lambda x:x[1],filter(lambda x:x[0] in fine_tune_dropoutnet_param_names,dn.named_parameters())))
    optimizer=optim.Adam([{'params': fine_tune_dropoutnet_params},
                          {'params': fine_tune_bert_params, 'lr': 1e-4/8}
                        ], lr=1e-4/8)
    return optimizer

#@title Learning rate adjust
def adjust_learning_rate(optimizer, epoch,rate=0.9):
    """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
    lr = (1e-4/8) * (rate ** epoch)
    optimizer.param_groups[1]['lr'] = lr
    # for param_group in optimizer.param_groups:
    #     param_group['lr'] = lr

#@title Plot
from matplotlib import pyplot as plt

def plot_train(losses,val=False):
    plt.title("{} Loss".format("Validation" if val else "Training"))
    plt.xlabel("Epoch")
    plt.ylabel("Relevance Loss")
    plt.plot(losses)
    plt.show()

#@title Training

from tqdm import tqdm
def evaluate(epoch,dn,val_dload,preds=None,tru=None):
    dn.eval()
    with torch.no_grad():
        eval_loss=0.0
        for (j,data) in tqdm(enumerate(val_dload,0)):

            u, v, u_input_ids, u_attention_mask, u_scalars, num_shows, v_input_ids, v_attention_mask, v_scalars, u_ids,v_ids, y = data
            if not j: 
                
                dn.update(v_ids)  
            v=np.zeros_like(v)
            u, v = torch.tensor(u,dtype=torch.float).to(device), torch.tensor(v,dtype=torch.float).to(device)
            u_input_ids, u_attention_mask, u_scalars, num_shows = u_input_ids.to(device), u_attention_mask.to(device), u_scalars.to(device), num_shows.to(device)
            # print(u_scalars.dtype)
            v_input_ids, v_attention_mask, v_scalars = v_input_ids.to(device), v_attention_mask.to(device), v_scalars.to(device)
            
            y = torch.tensor(y,dtype=torch.float).to(device)
            u_ids,v_ids=u_ids.numpy(),v_ids.numpy()
            r,loss=dn(u, v, u_input_ids, u_attention_mask, u_scalars, num_shows, v_input_ids, v_attention_mask, v_scalars, u_ids,v_ids,y,True)
            if preds!=None and tru!=None:
                preds.extend(r.cpu().numpy())
                tru.extend(y.cpu().numpy())
            loss=loss.mean()
            eval_loss+=loss.item()
            
        print('[%d] eval loss: %.5f' %(epoch + 1,  eval_loss / (j+1)))
        return eval_loss / (j+1)
            
def train(dn,optimizer,val_dload,num_epochs=100,cont=-1,num_accumulation_steps=8,save_every=25,save_path=""):
    

    ### PICK UP WHERE IT LEFT OFF
    
    preds=[]
    if os.path.isfile("{}_{}.pt".format(save_path,num_epochs-1)): 
        print("save_path","{}_{}.pt".format(save_path,num_epochs-1),"finished")
        return
    else:
        files=os.listdir(os.path.dirname(save_path))
        print(save_path)
        chkpts=[os.path.isfile("{}_{}.pt".format(save_path,save_every*i+save_every-1)) for i in range(num_epochs//save_every)]
        indmax=len(chkpts)-1-np.argmax(chkpts[::-1])
        if chkpts[indmax]:

            latest_ckpt="{}_{}.pt".format(save_path,save_every*indmax+save_every-1)
            cont=save_every*indmax+save_every-1
            print("latest ckpt",latest_ckpt)
            dn.load_state_dict(torch.load(latest_ckpt,map_location=device))
            print("continuing at",cont+1,"epoch")
        else:
            cont=-1
            print("training for",save_path)



    for epoch in range(cont+1,num_epochs):
        uaid=UserAnimeIDRandomAccess(u_dict,v_dict,u_feats,v_feats,sample_mask=sample_dic)
        dload=DataLoader(uaid,batch_size=4,shuffle=True,num_workers=2)
        dn.train(True)
        # data_iter=iter(dload) 
        epoch_loss=0.0
        if epoch >= 1:
            adjust_learning_rate(optimizer, epoch,rate=0.99)
        for (i,data) in enumerate(dload,0):
            u, v, u_input_ids, u_attention_mask, u_scalars, num_shows, v_input_ids, v_attention_mask, v_scalars, u_ids,v_ids,y = data
            dn.update(v_ids)   
            v=np.zeros_like(v)
            u, v = torch.tensor(u,dtype=torch.float).to(device), torch.tensor(v,dtype=torch.float).to(device)
            u_input_ids, u_attention_mask, u_scalars, num_shows = u_input_ids.to(device), u_attention_mask.to(device), u_scalars.to(device), num_shows.to(device)
            # print(u_scalars.dtype)
            v_input_ids, v_attention_mask, v_scalars = v_input_ids.to(device), v_attention_mask.to(device), v_scalars.to(device)
            
            y = torch.tensor(y,dtype=torch.float).to(device)

            r,loss=dn(u, v, u_input_ids, u_attention_mask, u_scalars, num_shows, v_input_ids, v_attention_mask, v_scalars, u_ids,v_ids,y)

            loss=loss.mean()
            s1=list(dn.parameters())[0].clone()

            loss.backward()
            
            if i%num_accumulation_steps==num_accumulation_steps-1:
                optimizer.step()
                optimizer.zero_grad()

            
            s2=list(dn.parameters())[0].clone()
            # assert i or (not torch.equal(s1.data,s2.data)) # only checks for i=0
            
            epoch_loss+=loss.item()
            torch.cuda.empty_cache()
        print('[%d] loss: %.5f' % (epoch + 1, epoch_loss / (i+1)))
        eval_loss=evaluate(epoch,dn,val_dload,preds)
        yield preds,epoch_loss/(i+1),eval_loss

        ###SAVE LATEST EPOCHS, BUT TFIDF TRAINS TOO FAST FOR IT TO SAVE EVERY LOSS
        ###SO JUST IGNORE FOR TFIDF, REMEMBER TO SAVE PLOTS
        with open("{}.txt".format(save_path,epoch),"a+") as f:
            f.write("{},{},{}\n".format(epoch,epoch_loss/(i+1),eval_loss))

        if epoch%save_every==save_every-1:
            torch.save(dn.state_dict(),"{}_{}.pt".format(save_path,epoch))
    

    
def run_experiment(dn,optimizer,uaid,val_uaid,num_epochs=100,plot_every=10,save_path=""):
    preds=None;epoch_losses=[];eval_losses=[]
    np.random.seed(0)
    random.seed(0)
  
    val_dload=DataLoader(val_uaid,batch_size=4,num_workers=2)
    for _ in range(num_epochs):
        
        for (preds,t_l,v_l) in train(dn,optimizer,val_dload,num_epochs,save_path=save_path):
            preds=preds
            epoch_losses.append(t_l)
            eval_losses.append(v_l)
            if len(epoch_losses)%plot_every==1:
                plot_train(epoch_losses)
            if len(epoch_losses)%plot_every==2:
                plot_train(eval_losses,True)

def checkpoint_name_from_config(path,config):
    #USAGE: path="models/tfidf_with_recs/"
    out=""
    for (k,v) in config.items():
        out+="{}={}__".format(k,v)
    out=out.rstrip("__")+".pt"
    return os.path.join(path,out)


###RUN EXPERIMENTS OVER ALL HPARAM CONFIGS
def run_experiments(configs,path="models/recsys/tfidf/recs"):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    for config in configs:
        save_path=checkpoint_name_from_config(path,config)
        os.makedirs(path,exist_ok=True)
        # SO COLAB PICKS UP WHERE IT LEFT OFF
        
        dn=DropoutTransformerNet(**config)
        dn=dn.to(device)
        optimizer=get_optimizer(dn)
        
        
        run_experiment(dn,optimizer,uaid,val_uaid,250,plot_every=3,save_path=save_path)

#@title UserAnimeID


class UserAnimeID(Dataset):
    def __init__(self,u_dict,v_dict,u_feats,v_feats):
        self.V_dict=v_dict
        self.U_dict=u_dict
        self.V_feats=v_feats 
        self.U_feats=u_feats
        self.V=np.array(list(v_dict.values()))
        self.U=np.array(list(u_dict.values()))
        self.V_phi=list(v_feats.values())
        self.U_phi=list(u_feats.values())
        self.labels=np.dot(self.U,self.V.T)#(num users,num items)
        self.num_users,self.num_items=self.labels.shape
        self.user_ids=list(map(int,u_dict.keys()))
        self.item_ids=list(map(int,v_dict.keys()))
    
    def __getitem__(self,idx): 
        item=idx%self.num_items 
        user=(idx-item)//self.num_items
        # u_id,v_id=self.user_item_pairs[idx]
        u_id,v_id=self.user_ids[user],self.item_ids[item]
        u,v,u_phi,v_phi=self.U_dict[u_id],self.V_dict[v_id],self.U_feats[u_id],self.V_feats[v_id]
        y=self.labels[user][item]
        return (u,v) + u_phi + v_phi + (u_id,v_id,)+(y,)

    def __len__(self): 
      return self.num_items * self.num_users

# uaid=UserAnimeID(u_dict,v_dict,u_feats,v_feats)
# val_uaid=UserAnimeID(u_dict,val_v_dict,u_feats,val_v_feats)

#@title UserAnimeCacheID

#now we only care about seeing every user and item at least once

class UserAnimeCacheID(UserAnimeID):
    def __init__(self,u_dict,v_dict,u_feats,v_feats):
        super().__init__(u_dict,v_dict,u_feats,v_feats)
        self.perimeter_idxes=list(range(len(v_dict)))+(len(v_dict)*np.arange(1,len(u_dict))).tolist()
    def __getitem__(self, idx):
        return super().__getitem__(self.perimeter_idxes[idx])
    def __len__(self):
        return len(self.perimeter_idxes)

#@title SlicedUserAnimeID


class SlicedUserAnimeID(UserAnimeID):
    def __init__(self,u_dict,v_dict,u_feats,v_feats,slice_users=10,slice_items=0):
        super().__init__(u_dict,v_dict,u_feats,v_feats)
        assert slice_users ^ slice_items
        self.slice_users=(slice_users>0)
        self.sliced_users=slice_users
        self.sliced_items=slice_items
        
    
    def __getitem__(self,idx): 
        prog=idx%(self.num_items if self.slice_users else self.num_users)
        dom=self.num_items if self.slice_users else self.num_users
        passes=(idx-prog)//dom
        #swap axis
        idx=passes*(self.num_items if self.slice_users else self.num_users)+prog
        return super().__getitem__(idx)

        
    def __len__(self): 
        return self.num_items*self.sliced_users if self.slice_users else self.num_users*self.sliced_items

list(enumerate(list(pg(config))))

#@title Eval
import heapq
def top_inds(relv,k,thresh=None):
    num_users,num_shows=relv.shape
    if thresh is not None:
      inds=[np.arange(num_users)[relv[:,x]>=thresh].tolist() for x in range(num_shows)]
    else:
      inds=[heapq.nlargest(k,np.arange(num_users),lambda i:relv[i][x]) for x in range(num_shows)]
    return inds 

def top_k_recall(val_preds,val_true,k=3,rating_thresh=1,thresh=0.5):
    NUM_USERS,NUM_ITEMS=val_true.shape
    k_rated=np.array(list(map(lambda x:len(x[x>=rating_thresh])>=k,val_true.T.values)))
    valid_inds=np.arange(NUM_ITEMS)[k_rated]
    
    relv_preds=val_preds.T[k_rated].T
    relv_true=val_true.iloc[:,k_rated]
    inds=top_inds(relv_true.values,k)
    pred_inds=top_inds(relv_preds,k,thresh)

    score=np.average([(len(true_)-len(set(true_).difference(set(pred_))))/len(set(true_)) for true_,pred_ in zip(inds,pred_inds)])
    pos_preds=np.average([len(pred_inds[i]) for i in range(len(pred_inds))])
    
    return score,pos_preds/NUM_USERS

def top_k_precision(val_preds,val_true,k=3,thresh=0.5):
    NUM_USERS,NUM_ITEMS=val_true.shape
    k_rated=np.array(list(map(lambda x:len(x[x>7.0])>=k,val_true.T.values)))
    valid_inds=np.arange(NUM_ITEMS)[k_rated]
    
    relv_preds=val_preds.T[k_rated].T
    relv_true=val_true.iloc[:,k_rated]
    print("got",relv_true.shape[1])
    inds=top_inds(relv_true.values,k)
    pred_inds=top_inds(relv_preds,k,thresh)
    score=np.average([len(set(pred_).intersection(set(true_)))/len(set(pred_)) for true_,pred_ in zip(inds,pred_inds) if len(set(pred_))>0])

def inv_rank(preds,labels):
    #preds,labels either (num_items) or (num_users)
    ind=np.argmax(labels)
    preds=sorted(np.arange(len(labels)),key=lambda i:preds[i])
    pos=np.argmax(preds==ind)
    return 1./pos

def mean_inv_rank(preds,labels,num_sliced=0):
    assert num_sliced
    preds,labels=np.array(preds),np.array(labels)
    #(sliced_users*num_items) or (sliced_items*num_users)
    num_per=preds.size//num_sliced
    assert num_per*num_sliced==preds.size 
    ans=np.average([inv_rank(preds[num_per*i:num_per*(i+1)],labels[num_per*i:num_per*(i+1)]) for i in range(num_sliced)])
    baseline=2./num_per
    return ans,baseline


def programmatic_recall_plot(preds,val_,mode='recall',rating_thresh=1,st=0.3,fin=0.7,num=3,num_=10,scale=10):
    assert mode in ['recall','precision']

    fig,axs=plt.subplots(1,num,figsize=(3*num,3),sharey=True,sharex=True)

    get=lambda k,b:(np.linspace(st,fin,num_),
            [top_k_recall(preds,val_,k=k,rating_thresh=rating_thresh,thresh=thresh)[int(b)] if mode=='recall' else 
             top_k_precision(preds,val_,k=k,rating_thresh=rating_thresh,thresh=thresh)[int(b)] for thresh in np.linspace(st,fin,num_)])
    
    axs[0].set_ylabel("%(Top-K users captured)")
    for i in range(num):
        axs[i].set_title('Top-(K={}) {}'.format(scale*(i+1),mode))
        axs[i].set_xlabel('Threshold')
        axs[i].plot(*get(scale*(i+1),0),'k')
        axs[i].plot(*get(scale*(i+1),1),'k--')
    plt.show()

def get_metrics(n_users,n_items,config):
    assert min(n_users,n_items)==0
    cache_uaid=UserAnimeCacheID(u_dict,val_v_dict,u_feats,val_v_feats)
    # sliced_uaid=SlicedUserAnimeID(u_dict,v_dict,u_feats,v_feats,slice_users=n_users,slice_items=n_items)
    dload=DataLoader(cache_uaid,batch_size=32,num_workers=16)
    preds,tru=[],[]
    
    dn=DropoutTransformerNet(**config)
    dn.load_state_dict(torch.load("models/recsys/deepanignet/anime_dim=774__bert=pretrained_finetune__data_mode=['tf-idf' 'bert-finetuned']__device=cuda:0__drop_p=0.75__edge_attr=True__hidden_dim=100__latent_dim=10__num_gnn_feats=32.pt_249.pt",map_location=device))
    dn=dn.to(device)
    evaluate(0,dn,dload)
    print("CACHED")
    # #AFTER CACHED, WE CAN COMPUTE WHAT WE WANT
    
    # sliced_uaid=SlicedUserAnimeID(u_dict,val_v_dict,u_feats,val_v_feats,slice_users=n_users,slice_items=n_items)
    # sliced_dload=DataLoader(sliced_uaid,batch_size=32,num_workers=16)
    # evaluate(0,dn,sliced_dload,preds,tru)
    # #(users, num_slice items) for user recall or (items, num_slice_users) for item recall
    # try:
    #     if n_items:
    #         preds__,tru__=np.array(preds).reshape(n_items,-1).T,pref_matrix.iloc[:,:n_items]
    #     else:
    #         preds__,tru__=np.array(preds).reshape(n_users,-1).T,pref_matrix.iloc[:n_users].T
    #     print("top k recall",top_k_recall(preds__,tru__,k=5,thresh=0.0))
    #     print("mean inv rank",mean_inv_rank(preds,tru,num_sliced=max(n_users,n_items)))
    #     # programmatic_recall_plot(preds__,tru__,'recall',-1.0,1.0,num=5,num_=100,scale=5)
    #     np.savetxt('inference/deepaninet_preds.txt',preds__,fmt='%s')
    #     np.savetxt('inference/deepaninet_tru.txt',tru__,fmt='%s')
    # except:
    #     pdb.set_trace()


    return dn


dn=get_metrics(0,1000,list(pg(config))[4])

#@title Metrics
#AFTER CACHED, WE CAN COMPUTE WHAT WE WANT

#this means we compute metrics on (n_items, all users) if n_users==0
n_users,n_items=0,1000

preds,tru=[],[]


#SWAP val_v_dict with v_dict and val_v_feats with v_feats for in-matrix metrics

# sliced_uaid=SlicedUserAnimeID(u_dict,val_v_dict,u_feats,val_v_feats,slice_users=n_users,slice_items=n_items)
# sliced_dload=DataLoader(sliced_uaid,batch_size=32,num_workers=16)
# evaluate(0,dn,sliced_dload,preds,tru)




#(users, num_slice items) for user recall or (items, num_slice_users) for item recall
# try:
#     if n_items:
#         preds__,tru__=np.array(preds).reshape(n_items,-1).T,pref_matrix.iloc[:,:n_items]
#     else:
#         preds__,tru__=np.array(preds).reshape(n_users,-1).T,pref_matrix.iloc[:n_users].T
#     print("top k recall",top_k_recall(preds__,tru__,k=5,thresh=-0.25))
#     print("mean inv rank",mean_inv_rank(preds,tru,num_sliced=max(n_users,n_items)))
#     programmatic_recall_plot(preds__,tru__,'recall',rating_thresh=7,st=-1.0,1.0,num=5,num_=100,scale=5)
#     np.savetxt('inference/deepaninet_preds.txt',preds__,fmt='%s')
#     np.savetxt('inference/deepaninet_tru.txt',tru__,fmt='%s')
# except:
#     pdb.set_trace()

U_hat=torch.stack([dn.dropoutgnet.f_u_cache[k] for k in u_dict])

V_hat=torch.stack([dn.dropoutgnet.f_v_cache[k] for k in val_v_dict])

R_hat=U_hat.matmul(V_hat.T)
R_hat_np=R_hat.cpu().numpy()

### ANDREW METRICS

#R_HAT_NP IS (NUM USERS, NUM VAL ITEMS) RELEVANCE SCORE MATRIX
#VAL_PREF_MATRIX IS (NUM USERS, NUM VAL ITEMS) RATING MATRIX

#So far, top_k_recall(rating_thresh, thresh) is defined as:
#(no. users with relevance>=thresh for item i)/(no. all users who rated >=rating_thresh for item i) ...averaged over all val items

#The paper defines item recall@M as: (no. articles the user likes and is top-M recommended)/(no. articles the user likes)
#The paper defines user recall@M as: (no. users the article is in their top-M recommended)/(no. users that like the article)

#^ IMPLEMENT THE TWO





programmatic_recall_plot(R_hat_np.T,val_pref_matrix.T,'recall',rating_thresh=5,st=-0.25,fin=2.0,num=10,num_=20,scale=5)

